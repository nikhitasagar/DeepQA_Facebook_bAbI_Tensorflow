{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "import os\n",
    "import gensim\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import operator\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.models.rnn.translate import seq2seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BabiDataset:\n",
    "    def __init__(self, data_dir, task_id, model_type, max_vocab_size=None):\n",
    "        self.task_id = task_id\n",
    "        self.data_dir = data_dir\n",
    "        self.model_type = model_type\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.vocab = set()\n",
    "        self.word_counter = dict()\n",
    "        self.raw_train_data = None\n",
    "        self.raw_test_data = None\n",
    "        self.num_tokens = None\n",
    "        \n",
    "        self.__load_data()\n",
    "        self.word2id_dict, self.id2word_dict = self.__create_word2id_dict()\n",
    "                \n",
    "        (self.train_input_raw, self.train_input_tokens, \n",
    "         self.train_labels_raw, self.train_labels_tokens, self.train_sentence_counts)  = self.__tokenize_sentences(self.raw_train_data)\n",
    "        \n",
    "        (self.test_input_raw, self.test_input_tokens, \n",
    "         self.test_labels_raw, self.test_labels_tokens, self.test_sentence_counts)  = self.__tokenize_sentences(self.raw_test_data)\n",
    "        \n",
    "        self.max_context_len = None\n",
    "        self.max_question_len = None\n",
    "\n",
    "    def __update_word_counter(self, sequence):\n",
    "        \"\"\" Update word_counter with counts for words in a sentence\n",
    "        \n",
    "        Args:\n",
    "            sequence (list<str>) : list of words in a sequence\n",
    "        \n",
    "        \"\"\"\n",
    "        for word in sequence:\n",
    "            self.word_counter[word] = self.word_counter.get(word, 0) + 1\n",
    "            \n",
    "    def __create_vocab(self):\n",
    "        \"\"\" Create set of most frequent unique words found in the training data \"\"\"\n",
    "        \n",
    "        if self.max_vocab_size == None:\n",
    "            self.vocab == set(self.word_counter.keys())\n",
    "        else:\n",
    "            self.vocab = set(sorted(self.word_counter, key=self.word_counter.get, reverse=True)[:self.max_vocab_size])\n",
    "        \n",
    "    def __parse_babi_file(self, txt_file):\n",
    "        with open(txt_file) as babi_file:\n",
    "            raw_data = []\n",
    "            curr_sample = None\n",
    "            for i, line in enumerate(open(txt_file)):\n",
    "                id = int(line[0:line.find(' ')])\n",
    "                if id == 1:\n",
    "                    skip = False\n",
    "                    curr_sample = {\"C\": [], \"Q\": \"\", \"A\": \"\"}\n",
    "\n",
    "                line = line.strip()\n",
    "                line = line.replace('.', ' . ')\n",
    "                line = line[line.find(' ') + 1:]\n",
    "                self.__update_word_counter(line)\n",
    "                if line.find('?') == -1:\n",
    "                    curr_sample[\"C\"].append(line)\n",
    "                else:\n",
    "                    idx = line.find('?')\n",
    "                    tmp = line[idx + 1:].split('\\t')\n",
    "                    curr_sample[\"Q\"] = line[:idx]\n",
    "                    curr_sample[\"A\"] = tmp[1].strip()\n",
    "                    raw_data.append(deepcopy(curr_sample))\n",
    "\n",
    "            self.__create_vocab()\n",
    "            print \"Loaded {} data samples from {}\".format(len(raw_data), txt_file.split(self.data_dir)[1])\n",
    "            return raw_data\n",
    "\n",
    "    def __load_data(self):\n",
    "        files = [os.path.join(self.data_dir, f) for f in os.listdir(self.data_dir)]\n",
    "        s = 'qa{}_'.format(self.task_id)\n",
    "        task_files = [f for f in files if s in f]\n",
    "        train_file = [f for f in task_files if 'train' in f][0]\n",
    "        test_file = [f for f in task_files if 'test' in f][0] \n",
    "        \n",
    "        self.raw_train_data = self.__parse_babi_file(train_file)\n",
    "        self.raw_test_data = self.__parse_babi_file(test_file)\n",
    "        \n",
    "#    def __create_unique_word_corpus(self, raw_data_dict):\n",
    "#        all_words = []\n",
    "#        for x in raw_data_dict:\n",
    "#            for fact in x[\"C\"]:\n",
    "#                for word in fact.lower().split(' '):\n",
    "#                    if len(word) > 0:\n",
    "#                         all_words.append(word)\n",
    "\n",
    "#             for word in x[\"Q\"].lower().split(' '):\n",
    "#                 if len(word) > 0:\n",
    "#                     all_words.append(word)\n",
    "#             for word in x[\"A\"].lower().split(' '):\n",
    "#                 if len(word) > 0:\n",
    "#                     all_words.append(word)\n",
    "#         word_corpus = set(all_words)\n",
    "#         print \"{} unique words found\".format(len(word_corpus))\n",
    "#         return word_corpus\n",
    "    \n",
    "    def __create_word2id_dict(self):\n",
    "        word2id_dict = dict()\n",
    "        #self.vocab = self.__create_unique_word_corpus(self.raw_train_data)\n",
    "        \n",
    "        if self.model_type == 'LSTM':\n",
    "            word2id_dict['PAD'] = 0\n",
    "            word2id_dict['UNK'] = 1\n",
    "        elif self.model_type == 'seq2seq':\n",
    "            word2id_dict['PAD'] = 0\n",
    "            word2id_dict['GO'] = 1\n",
    "            word2id_dict['EOS'] = 2\n",
    "            word2id_dict['UNK'] = 3\n",
    "            word2id_dict['Q'] = 4\n",
    "        elif self.model_type == 'DMN':\n",
    "            word2id_dict['PAD'] = 0\n",
    "            word2id_dict['UNK'] = 1\n",
    "        else:\n",
    "            print \"Error: Model type {} invalid\".format(self.model_type)\n",
    "            \n",
    "        for word in self.vocab:\n",
    "            word2id_dict[word] = len(word2id_dict)\n",
    "\n",
    "        id2word_dict = dict(zip(word2id_dict.values(), word2id_dict.keys()))\n",
    "        self.num_tokens = len(word2id_dict)\n",
    "        return word2id_dict, id2word_dict\n",
    "    \n",
    "    def __convert_word2id(self, word):\n",
    "        try:\n",
    "            word_id = self.word2id_dict[word]\n",
    "        except:\n",
    "            word_id = self.word2id_dict['UNK']\n",
    "        return word_id\n",
    "\n",
    "    def __convert_to_one_hot(self, labels):\n",
    "        one_hot = np.array([[0 for j in range(self.num_tokens)] for i in range(len(labels))])\n",
    "\n",
    "        for i in range(len(one_hot)):\n",
    "            one_hot[i][labels[i]] = 1\n",
    "\n",
    "        return one_hot\n",
    "    \n",
    "    def __tokenize_sentences(self, raw_data):\n",
    "        \"\"\" Tokenizes sentences.\n",
    "        :param raw: dict returned from load_babi\n",
    "        :param word_table: WordTable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        context = []\n",
    "        context_ids = []\n",
    "        questions = []\n",
    "        question_ids = []\n",
    "        answers = []\n",
    "        answer_ids = []\n",
    "        sentence_counts = []\n",
    "\n",
    "        if self.model_type == 'LSTM':\n",
    "            for sample in raw_data:\n",
    "                story = []\n",
    "                story_ids = []\n",
    "                for sentence in sample[\"C\"]:\n",
    "                    seq = [w for w in sentence.lower().split(' ') if len(w) > 0]\n",
    "                    seq_ids = [self.__convert_word2id(w) for w in sentence.lower().split(' ') if len(w) > 0]\n",
    "                    story.append(seq)\n",
    "                    story_ids.append(seq_ids)\n",
    "\n",
    "                q = [w for w in sample[\"Q\"].lower().split(' ') if len(w) > 0]\n",
    "                q_ids = [self.__convert_word2id(w) for w in sample[\"Q\"].lower().split(' ') if len(w) > 0]\n",
    "\n",
    "                context.append([word for sentence in story for word in sentence])\n",
    "                context_ids.append([word_id for sentence in story_ids for word_id in sentence])\n",
    "\n",
    "                questions.append(q)\n",
    "                question_ids.append(q_ids)\n",
    "\n",
    "                answers.append(sample[\"A\"])  # NOTE: here we assume the answer is one word!\n",
    "                answer_ids.append(self.__convert_word2id(sample[\"A\"]))\n",
    "                sentence_counts.append(len(story))\n",
    "            answer_ids = self.__convert_to_one_hot(answer_ids)\n",
    "            context_ids, question_ids, answer_ids = np.array(context_ids), np.array(question_ids), np.array(answer_ids)\n",
    "            packaged_data = zip(context, context_ids, questions, question_ids, answers, answer_ids, sentence_counts)\n",
    "            random.shuffle(packaged_data)\n",
    "            context, context_ids, questions, question_ids, answers, answer_ids, sentence_counts = zip(*packaged_data)\n",
    "            return zip(context, questions), zip(context_ids, question_ids), answers, answer_ids, sentence_counts\n",
    "            \n",
    "        elif self.model_type == 'seq2seq':\n",
    "            for sample in raw_data:\n",
    "                story = []\n",
    "                story_ids = []\n",
    "                for sentence in sample[\"C\"]:\n",
    "                    seq = [w for w in sentence.lower().split(' ') if len(w) > 0]\n",
    "                    seq_ids = [self.__convert_word2id(w) for w in sentence.lower().split(' ') if len(w) > 0]\n",
    "                    story.append(seq)\n",
    "                    story_ids.append(seq_ids)\n",
    "\n",
    "                q = [w for w in sample[\"Q\"].lower().split(' ') if len(w) > 0]\n",
    "                q_ids = [self.__convert_word2id(w) for w in sample[\"Q\"].lower().split(' ') if len(w) > 0]\n",
    "\n",
    "                context.append([word for sentence in story for word in sentence] + ['Q'] + q)\n",
    "                context_ids.append([word_id for sentence in story_ids for word_id in sentence] + [self.word2id_dict['Q']] + q_ids)\n",
    "\n",
    "                answers.append(['GO'] + sample[\"A\"].lower().split(' ') + ['EOS'])\n",
    "                answer_ids.append([self.word2id_dict['GO']] + [self.__convert_word2id(w) for w in sample[\"A\"].lower().split(' ')] + [self.word2id_dict['EOS']])\n",
    "                sentence_counts.append(len(story))    \n",
    "            context_ids, answer_ids = np.array(context_ids), np.array(answer_ids)\n",
    "            packaged_data = zip(context, context_ids, answers, answer_ids, sentence_counts)\n",
    "            random.shuffle(packaged_data)\n",
    "            context, context_ids, answers, answer_ids, sentence_counts = zip(*packaged_data)\n",
    "            return context, context_ids, answers, answer_ids, sentence_counts\n",
    "            \n",
    "        elif model_type == 'DMN':\n",
    "            for sample in raw_data:\n",
    "                story = []\n",
    "                story_ids = []\n",
    "                for sentence in sample[\"C\"]:\n",
    "                    seq = [w for w in sentence.lower().split(' ') if len(w) > 0]\n",
    "                    seq_ids = [self.__convert_word2id(w) for w in sentence.lower().split(' ') if len(w) > 0]\n",
    "                    story.append(seq)\n",
    "                    story_ids.append(seq_ids)\n",
    "\n",
    "                q = [w for w in sample[\"Q\"].lower().split(' ') if len(w) > 0]\n",
    "                q_ids = [self.__convert_word2id(w) for w in sample[\"Q\"].lower().split(' ') if len(w) > 0]\n",
    "\n",
    "                context.append(story)\n",
    "                context_ids.append(story_ids)\n",
    "\n",
    "                questions.append(q)\n",
    "                question_ids.append(q_ids)\n",
    "\n",
    "                answers.append(sample[\"A\"])  # NOTE: here we assume the answer is one word!\n",
    "                answer_ids.append(self.__convert_word2id(sample[\"A\"]))\n",
    "                sentence_counts.append(len(story))\n",
    "            answer_ids = self.__convert_to_one_hot(answer_ids)\n",
    "            context_ids, question_ids, answer_ids = np.array(context_ids), np.array(question_ids), np.array(answer_ids)\n",
    "            packaged_data = zip(context, context_ids, questions, question_ids, answers, answer_ids, sentence_counts)\n",
    "            random.shuffle(packaged_data)\n",
    "            context, context_ids, questions, question_ids, answers, answer_ids, sentence_counts = zip(*packaged_data)\n",
    "            return zip(context, questions), zip(context_ids, question_ids), answers, answer_ids, sentence_counts\n",
    "\n",
    "    def __get_max_sequence_length(self, sequences):\n",
    "        max_len = 0\n",
    "        min_len = 1000\n",
    "        avg_len = 0\n",
    "        for sequence in sequences:\n",
    "            max_len = max(max_len, len(sequence))\n",
    "            min_len = min(min_len, len(sequence))\n",
    "            avg_len += len(sequence)\n",
    "        avg_len = int(float(avg_len) / len(sequences))\n",
    "        return max_len, min_len, avg_len\n",
    "    \n",
    "    def __apply_padding(self, sequences, length):\n",
    "        padded_data = []\n",
    "        \n",
    "        for id_sequence in sequences:\n",
    "            if len(id_sequence) < length:\n",
    "                padded_sequence = id_sequence\n",
    "                for i in range(length - len(id_sequence)):\n",
    "                    padded_sequence.append(0)\n",
    "                padded_data.append(padded_sequence)\n",
    "            elif len(id_sequence) > length:\n",
    "                clipped_sequence = id_sequence[:length]\n",
    "                padded_data.append(clipped_sequence)\n",
    "            else:\n",
    "                padded_data.append(id_sequence)\n",
    "        return padded_data\n",
    "\n",
    "    def pad_sequences(self, pad_lengths=None):\n",
    "        if self.model_type == 'LSTM':\n",
    "            train_context_data, train_question_data = zip(*self.train_input_tokens)\n",
    "            test_context_data, test_question_data = zip(*self.test_input_tokens)\n",
    "            \n",
    "            if pad_lengths == None:\n",
    "                self.max_context_len, min_context_len, avg_context_len = self._get_max_sequence_length(train_context_data)\n",
    "                self.max_question_len, min_question_len, avg_question_len = self._get_max_sequence_length(train_question_data)\n",
    "                print 'Context Lengths: max = {}, min = {}, avg = {}'.format(self.max_context_len, min_context_len, avg_context_len)\n",
    "                print 'Question Lengths: max = {}, min = {}, avg = {}'.format(self.max_question_len, min_question_len, avg_question_len)\n",
    "            elif len(pad_lengths) == 2:\n",
    "                self.max_context_len = pad_lengths[0]\n",
    "                self.max_question_len = pad_lengths[1]\n",
    "            else:\n",
    "                print \"Error: pad_lengths needs form [context_len, question_len]\"\n",
    "\n",
    "            train_context_data = self._apply_padding(train_context_data, self.max_context_len)\n",
    "            train_question_data = self._apply_padding(train_question_data, self.max_question_len)\n",
    "            test_context_data = self._apply_padding(test_context_data, self.max_context_len)\n",
    "            test_question_data = self._apply_padding(test_question_data, self.max_question_len)\n",
    "\n",
    "            self.train_input_tokens = zip(train_context_data, train_question_data)\n",
    "            self.test_input_tokens = zip(test_context_data, test_question_data)   \n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataIterator:\n",
    "    def __init__(self, data, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.iter = self.make_random_iter()\n",
    "        \n",
    "    def next_batch(self):\n",
    "        try:\n",
    "            idxs = self.iter.next()\n",
    "        except StopIteration:\n",
    "            self.iter = self.make_random_iter()\n",
    "            idxs = self.iter.next()\n",
    "        X, y = zip(*[self.data[i] for i in idxs])\n",
    "        return X, y\n",
    "\n",
    "    def make_random_iter(self):\n",
    "        splits = np.arange(self.batch_size, len(self.data), self.batch_size)\n",
    "        it = np.split(np.random.permutation(range(len(self.data))), splits)[:-1]\n",
    "        return iter(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Baseline LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/media/ai2-bb8/data_disk/data_sets/bAbI/tasks_1-20_v1-2/en-10k/'\n",
    "LSTM_data = BabiDataset(data_dir, 20, 'LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Lengths: max = 69, min = 4, avg = 29\n",
      "Question Lengths: max = 7, min = 4, avg = 5\n"
     ]
    }
   ],
   "source": [
    "LSTM_data.pad_sequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = zip(LSTM_data.train_input_tokens, LSTM_data.train_labels_tokens)\n",
    "test_data = zip(LSTM_data.test_input_tokens, LSTM_data.test_labels_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_iter = DataIterator(train_data, 128)\n",
    "test_data_iter = DataIterator(test_data, 999)\n",
    "deploy_data_iter = DataIterator(test_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:1'):\n",
    "    # Parameters\n",
    "    learning_rate = 0.1\n",
    "    training_iters = 300000\n",
    "    batch_size = 128\n",
    "    display_step = 1000\n",
    "    test_interval = 5000\n",
    "    vocab_size = LSTM_data.num_tokens\n",
    "\n",
    "    # Network Parameters\n",
    "    word_dim = 64 # word vector dimensions\n",
    "    n_steps_question = LSTM_data.max_question_len # timesteps\n",
    "    n_steps_story = LSTM_data.max_context_len\n",
    "    n_hidden = 128 # hidden layer num of features\n",
    "    n_classes = vocab_size # total classes\n",
    "\n",
    "    X_story = tf.placeholder(tf.int32, [None, n_steps_story])\n",
    "    X_question = tf.placeholder(tf.int32, [None, n_steps_question])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'word_embeddings': tf.Variable(tf.random_uniform([vocab_size, word_dim], -1.0, 1.0)),\n",
    "        'output': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'output': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    def model(X_story, X_question):\n",
    "        \n",
    "        ### story LSTM ###\n",
    "        story_word_embeddings = tf.nn.embedding_lookup(weights['word_embeddings'], X_story)\n",
    "        story_word_embeddings = tf.nn.dropout(story_word_embeddings, 0.3)\n",
    "        # Prepare data shape to match `rnn` function requirements\n",
    "        # Current data input shape: (batch_size, n_steps, n_input)\n",
    "        # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "        # Permuting batch_size and n_steps\n",
    "        story_word_embeddings = tf.transpose(story_word_embeddings, [1, 0, 2])\n",
    "        # Reshaping to (n_steps*batch_size, n_input)\n",
    "        story_word_embeddings = tf.reshape(story_word_embeddings, [-1, word_dim])\n",
    "        # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "        story_word_embeddings = tf.split(0, n_steps_story, story_word_embeddings)\n",
    "        \n",
    "        with tf.variable_scope('story_LSTM'):\n",
    "            # Define a lstm cell with tensorflow\n",
    "            story_lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "\n",
    "            # Get lstm cell output\n",
    "            story_outputs, story_states = rnn.rnn(story_lstm_cell, story_word_embeddings, dtype=tf.float32)\n",
    "        \n",
    "        ### question LSTM ###\n",
    "        question_word_embeddings = tf.nn.embedding_lookup(weights['word_embeddings'], X_question)\n",
    "        question_word_embeddings = tf.nn.dropout(question_word_embeddings, 0.3)\n",
    "        \n",
    "        question_word_embeddings = tf.transpose(question_word_embeddings, [1, 0, 2])\n",
    "        question_word_embeddings = tf.reshape(question_word_embeddings, [-1, word_dim])\n",
    "        question_word_embeddings = tf.split(0, n_steps_question, question_word_embeddings)\n",
    "\n",
    "        with tf.variable_scope('question_LSTM'):\n",
    "            question_lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "\n",
    "            question_outputs, question_states = rnn.rnn(question_lstm_cell, question_word_embeddings, dtype=tf.float32)\n",
    "        \n",
    "        # sum question and story vectors\n",
    "        combined_vector = tf.add(story_outputs[-1], question_outputs[-1])\n",
    "        \n",
    "        # return dense layer\n",
    "        return tf.matmul(combined_vector, weights['output']) + biases['output']\n",
    "\n",
    "    pred = model(X_story, X_question)\n",
    "    softmax = tf.nn.softmax(pred)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1000, Minibatch Loss= 0.492414, Training Accuracy= 0.73438\n",
      "Iter 2000, Minibatch Loss= 0.613970, Training Accuracy= 0.76562\n",
      "Iter 3000, Minibatch Loss= 0.313448, Training Accuracy= 0.81250\n",
      "Iter 4000, Minibatch Loss= 0.290781, Training Accuracy= 0.91406\n",
      "Iter 5000, Minibatch Loss= 0.139194, Training Accuracy= 0.87500\n",
      "('Testing Accuracy:', 0.8998999)\n",
      " \n",
      "STORY\n",
      "sumit is tired . jason is bored . sumit moved to the bedroom . sumit took the pajamas there . antoine is tired . antoine journeyed to the bedroom . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      " \n",
      "QUESTION\n",
      "why did antoine go to the bedroom\n",
      " \n",
      "GROUND TRUTH\n",
      "tired\n",
      " \n",
      "MODEL PREDICTION\n",
      "tired\n",
      " \n",
      "Iter 6000, Minibatch Loss= 0.172792, Training Accuracy= 0.89844\n",
      "Iter 7000, Minibatch Loss= 0.404511, Training Accuracy= 0.91406\n",
      "Iter 8000, Minibatch Loss= 0.204393, Training Accuracy= 0.85938\n",
      "Iter 9000, Minibatch Loss= 0.250061, Training Accuracy= 0.88281\n",
      "Iter 10000, Minibatch Loss= 0.190397, Training Accuracy= 0.89062\n",
      "('Testing Accuracy:', 0.9009009)\n",
      " \n",
      "STORY\n",
      "jason is thirsty . yann is bored . jason moved to the kitchen . jason grabbed the milk there . antoine is tired . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      " \n",
      "QUESTION\n",
      "where will antoine go PAD PAD PAD\n",
      " \n",
      "GROUND TRUTH\n",
      "bedroom\n",
      " \n",
      "MODEL PREDICTION\n",
      "bedroom\n",
      " \n",
      "Iter 11000, Minibatch Loss= 0.194226, Training Accuracy= 0.92188\n",
      "Iter 12000, Minibatch Loss= 0.286024, Training Accuracy= 0.85938\n",
      "Iter 13000, Minibatch Loss= 0.162673, Training Accuracy= 0.89062\n",
      "Iter 14000, Minibatch Loss= 0.159688, Training Accuracy= 0.92188\n",
      "Iter 15000, Minibatch Loss= 0.212420, Training Accuracy= 0.86719\n",
      "('Testing Accuracy:', 0.8998999)\n",
      " \n",
      "STORY\n",
      "antoine is tired . jason is hungry . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      " \n",
      "QUESTION\n",
      "where will jason go PAD PAD PAD\n",
      " \n",
      "GROUND TRUTH\n",
      "kitchen\n",
      " \n",
      "MODEL PREDICTION\n",
      "kitchen\n",
      " \n",
      "Iter 16000, Minibatch Loss= 0.133443, Training Accuracy= 0.95312\n",
      "Iter 17000, Minibatch Loss= 0.132269, Training Accuracy= 0.92969\n",
      "Iter 18000, Minibatch Loss= 0.316372, Training Accuracy= 0.96094\n",
      "Iter 19000, Minibatch Loss= 0.163752, Training Accuracy= 0.91406\n",
      "Iter 20000, Minibatch Loss= 0.128717, Training Accuracy= 0.87500\n",
      "('Testing Accuracy:', 0.8958959)\n",
      " \n",
      "STORY\n",
      "antoine is hungry . antoine went to the kitchen . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      " \n",
      "QUESTION\n",
      "why did antoine go to the kitchen\n",
      " \n",
      "GROUND TRUTH\n",
      "hungry\n",
      " \n",
      "MODEL PREDICTION\n",
      "thirsty\n",
      " \n",
      "Iter 21000, Minibatch Loss= 0.262122, Training Accuracy= 0.89062\n",
      "Iter 22000, Minibatch Loss= 0.137239, Training Accuracy= 0.92188\n",
      "Iter 23000, Minibatch Loss= 0.310654, Training Accuracy= 0.92188\n",
      "Iter 24000, Minibatch Loss= 0.572923, Training Accuracy= 0.92188\n",
      "Iter 25000, Minibatch Loss= 0.227629, Training Accuracy= 0.89062\n",
      "('Testing Accuracy:', 0.8958959)\n",
      " \n",
      "STORY\n",
      "antoine is tired . antoine went back to the bedroom . jason is hungry . sumit is tired . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      " \n",
      "QUESTION\n",
      "where will sumit go PAD PAD PAD\n",
      " \n",
      "GROUND TRUTH\n",
      "bedroom\n",
      " \n",
      "MODEL PREDICTION\n",
      "bedroom\n",
      " \n",
      "Iter 26000, Minibatch Loss= 0.153218, Training Accuracy= 0.90625\n",
      "Iter 27000, Minibatch Loss= 0.145871, Training Accuracy= 0.96094\n",
      "Iter 28000, Minibatch Loss= 0.161894, Training Accuracy= 0.93750\n",
      "Iter 29000, Minibatch Loss= 0.145277, Training Accuracy= 0.92188\n",
      "Iter 30000, Minibatch Loss= 0.198124, Training Accuracy= 0.92188\n",
      "('Testing Accuracy:', 0.8918919)\n",
      " \n",
      "STORY\n",
      "antoine is tired . jason is hungry . jason journeyed to the kitchen . sumit is bored . sumit travelled to the garden . jason got the apple there . antoine went back to the bedroom . yann is bored . yann went to the garden . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      " \n",
      "QUESTION\n",
      "why did yann go to the garden\n",
      " \n",
      "GROUND TRUTH\n",
      "bored\n",
      " \n",
      "MODEL PREDICTION\n",
      "bored\n",
      " \n",
      "Iter 31000, Minibatch Loss= 0.207295, Training Accuracy= 0.87500\n",
      "Iter 32000, Minibatch Loss= 0.240147, Training Accuracy= 0.87500\n",
      "Iter 33000, Minibatch Loss= 0.176535, Training Accuracy= 0.92188\n",
      "Iter 34000, Minibatch Loss= 0.166586, Training Accuracy= 0.88281\n",
      "Iter 35000, Minibatch Loss= 0.289454, Training Accuracy= 0.89844\n",
      "('Testing Accuracy:', 0.91591591)\n",
      " \n",
      "STORY\n",
      "jason is thirsty . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      " \n",
      "QUESTION\n",
      "where will jason go PAD PAD PAD\n",
      " \n",
      "GROUND TRUTH\n",
      "kitchen\n",
      " \n",
      "MODEL PREDICTION\n",
      "kitchen\n",
      " \n",
      "Iter 36000, Minibatch Loss= 0.152145, Training Accuracy= 0.94531\n",
      "Iter 37000, Minibatch Loss= 0.170540, Training Accuracy= 0.82031\n",
      "Iter 38000, Minibatch Loss= 0.213912, Training Accuracy= 0.95312\n",
      "Iter 39000, Minibatch Loss= 0.124788, Training Accuracy= 0.95312\n",
      "Iter 40000, Minibatch Loss= 0.189449, Training Accuracy= 0.87500\n",
      "('Testing Accuracy:', 0.9039039)\n",
      " \n",
      "STORY\n",
      "jason is thirsty . antoine is bored . jason went to the kitchen . antoine journeyed to the garden . sumit is hungry . antoine grabbed the football there . sumit went to the kitchen . jason took the milk there . sumit took the apple there . yann is thirsty . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      " \n",
      "QUESTION\n",
      "where will yann go PAD PAD PAD\n",
      " \n",
      "GROUND TRUTH\n",
      "kitchen\n",
      " \n",
      "MODEL PREDICTION\n",
      "kitchen\n",
      " \n",
      "Iter 41000, Minibatch Loss= 0.156120, Training Accuracy= 0.89844\n",
      "Iter 42000, Minibatch Loss= 0.391561, Training Accuracy= 0.89062\n",
      "Iter 43000, Minibatch Loss= 0.117059, Training Accuracy= 0.90625\n",
      "Iter 44000, Minibatch Loss= 0.173426, Training Accuracy= 0.94531\n",
      "Iter 45000, Minibatch Loss= 0.197803, Training Accuracy= 0.86719\n",
      "('Testing Accuracy:', 0.9089089)\n",
      " \n",
      "STORY\n",
      "yann is thirsty . antoine is bored . antoine journeyed to the garden . antoine picked up the football there . jason is thirsty . sumit is bored . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      " \n",
      "QUESTION\n",
      "where will sumit go PAD PAD PAD\n",
      " \n",
      "GROUND TRUTH\n",
      "garden\n",
      " \n",
      "MODEL PREDICTION\n",
      "garden\n",
      " \n",
      "Iter 46000, Minibatch Loss= 0.411097, Training Accuracy= 0.86719\n",
      "Iter 47000, Minibatch Loss= 0.215056, Training Accuracy= 0.90625\n",
      "Iter 48000, Minibatch Loss= 0.277813, Training Accuracy= 0.88281\n",
      "Iter 49000, Minibatch Loss= 0.266417, Training Accuracy= 0.86719\n",
      "Iter 50000, Minibatch Loss= 0.230963, Training Accuracy= 0.83594\n",
      "('Testing Accuracy:', 0.9099099)\n",
      " \n",
      "STORY\n",
      "jason is thirsty . sumit is bored . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      " \n",
      "QUESTION\n",
      "where will sumit go PAD PAD PAD\n",
      " \n",
      "GROUND TRUTH\n",
      "garden\n",
      " \n",
      "MODEL PREDICTION\n",
      "garden\n",
      " \n",
      "Iter 51000, Minibatch Loss= 0.214771, Training Accuracy= 0.87500\n",
      "Iter 52000, Minibatch Loss= 0.126037, Training Accuracy= 0.94531\n",
      "Iter 53000, Minibatch Loss= 0.228334, Training Accuracy= 0.94531\n",
      "Iter 54000, Minibatch Loss= 0.146337, Training Accuracy= 0.89844\n",
      "Iter 55000, Minibatch Loss= 0.154582, Training Accuracy= 0.89844\n",
      "('Testing Accuracy:', 0.9029029)\n",
      " \n",
      "STORY\n",
      "sumit is tired . jason is thirsty . sumit went back to the bedroom . jason moved to the kitchen . sumit got the pajamas there . antoine is tired . antoine went to the bedroom . yann is bored . yann journeyed to the garden . jason took the milk there . yann got the football there . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      " \n",
      "QUESTION\n",
      "why did yann get the football PAD\n",
      " \n",
      "GROUND TRUTH\n",
      "bored\n",
      " \n",
      "MODEL PREDICTION\n",
      "bored\n",
      " \n",
      "Iter 56000, Minibatch Loss= 0.198842, Training Accuracy= 0.90625\n",
      "Iter 57000, Minibatch Loss= 0.123298, Training Accuracy= 0.88281\n",
      "Iter 58000, Minibatch Loss= 0.162078, Training Accuracy= 0.90625\n",
      "Iter 59000, Minibatch Loss= 0.128948, Training Accuracy= 0.91406\n",
      "Iter 60000, Minibatch Loss= 0.151458, Training Accuracy= 0.89844\n",
      "('Testing Accuracy:', 0.8938939)\n",
      " \n",
      "STORY\n",
      "antoine is tired . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      " \n",
      "QUESTION\n",
      "where will antoine go PAD PAD PAD\n",
      " \n",
      "GROUND TRUTH\n",
      "bedroom\n",
      " \n",
      "MODEL PREDICTION\n",
      "bedroom\n",
      " \n",
      "Iter 61000, Minibatch Loss= 0.126263, Training Accuracy= 0.91406\n",
      "Iter 62000, Minibatch Loss= 0.155698, Training Accuracy= 0.92188\n",
      "Iter 63000, Minibatch Loss= 0.209712, Training Accuracy= 0.93750\n",
      "Iter 64000, Minibatch Loss= 0.197883, Training Accuracy= 0.92188\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-97e2339932aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# Run optimization op (backprop)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX_story\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_story_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_question\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_question_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtrain_iter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ai2-bb8/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 710\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    711\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ai2-bb8/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 908\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ai2-bb8/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 958\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    959\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/ai2-bb8/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    963\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ai2-bb8/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Keep training until reach max iterations\n",
    "    for train_iter in range(training_iters):\n",
    "        train_iter += 1\n",
    "        \n",
    "        X_batch, y_batch = train_data_iter.next_batch()\n",
    "        X_story_batch, X_question_batch = zip(*X_batch)\n",
    "\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={X_story: X_story_batch, X_question: X_question_batch, y: y_batch})\n",
    "        \n",
    "        if train_iter % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={X_story: X_story_batch, X_question: X_question_batch, y: y_batch})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={X_story: X_story_batch, X_question: X_question_batch, y: y_batch})\n",
    "            print(\"Iter \" + str(train_iter) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "            \n",
    "        if train_iter % test_interval == 0:\n",
    "            # Calculate accuracy on full test dataset\n",
    "            X_batch, y_batch = test_data_iter.next_batch()\n",
    "            X_story_batch, X_question_batch = zip(*X_batch)\n",
    "            \n",
    "            print(\"Testing Accuracy:\", \\\n",
    "                sess.run(accuracy, feed_dict={X_story: X_story_batch, X_question: X_question_batch, y: y_batch}))\n",
    "            \n",
    "            X_batch, y_batch = deploy_data_iter.next_batch()\n",
    "            X_story_batch, X_question_batch = zip(*X_batch)\n",
    "            deploy_outputs = sess.run(softmax, feed_dict={X_story: X_story_batch, X_question: X_question_batch, y: y_batch})\n",
    "            deploy_pred = LSTM_data.id2word_dict[int(np.argmax(deploy_outputs))]\n",
    "            \n",
    "            print ' '\n",
    "            print 'STORY'\n",
    "            print ' '.join([LSTM_data.id2word_dict[idx] for idx in X_story_batch[0]])\n",
    "            print ' '\n",
    "            print 'QUESTION'\n",
    "            print ' '.join([LSTM_data.id2word_dict[idx] for idx in X_question_batch[0]])\n",
    "            print ' '\n",
    "            print 'GROUND TRUTH'\n",
    "            print LSTM_data.id2word_dict[list(y_batch[0]).index(1)] \n",
    "            print ' '\n",
    "            print 'MODEL PREDICTION'\n",
    "            print deploy_pred\n",
    "            print ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## LSTM \n",
    "# task 1 - 51.3%\n",
    "# task 2 - 17.1%\n",
    "# task 3 - 21.7%\n",
    "# task 4 - 51.1%\n",
    "# task 5 - 80.1%\n",
    "# task 6 - 50.6%\n",
    "# task 7 - 62.6%\n",
    "# task 8 - 33.5%\n",
    "# task 9 - 63.3%\n",
    "# task 10 - 43.9%\n",
    "# task 11 - 63.6%\n",
    "# task 12 - 75.5%\n",
    "# task 13 - 93.9%\n",
    "# task 14 - 20.9%\n",
    "# task 15 - 25.1%\n",
    "# task 16 - 48.7%\n",
    "# task 17 - 52.2%\n",
    "# task 18 - 90.6%\n",
    "# task 19 - 8.5%\n",
    "# task 20 - 90.9%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
