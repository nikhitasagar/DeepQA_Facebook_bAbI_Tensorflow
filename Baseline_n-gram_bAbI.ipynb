{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sha1:ccd3c0a10888:c93dea2ba57a49f2f27a006e5e52a94bff9ec2f6'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from IPython.lib import passwd\n",
    "password = passwd('ai2')\n",
    "password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class example_ind(object):\n",
    "    def __init__(self, sentences, mask, question, answer, hints):\n",
    "        '''\n",
    "            Object which contains relevant information for inputting into the\n",
    "            model, but whose elements are integer indicies into a word vector\n",
    "            matrix.\n",
    "        '''\n",
    "        self.sentences = sentences  # stored as a matrix, rows as sentences. Cols are zero-padded\n",
    "        self.mask = mask            # boolean matrix M_{ij} = 1 if word j is in sentence i (0 for padding)\n",
    "        self.question = question    # vector of indices\n",
    "        self.answer = answer        # vector of indices\n",
    "        self.hints = hints          # 0-1 vector equal to the length of the # of sentences. 1 iff sentence i is relevant\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\"Training example: \\n\\t Info: %s \\n\\t Question: %s \\n\\t Answer: %s \\n\\t Hint: %s \\n\"\n",
    "                % (self.sentences, self.question, self.answer, self.hints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class example(object):\n",
    "    def __init__(self, sentences, question, answer, hints):\n",
    "        '''\n",
    "            Object which contains relevant information for inputting into the\n",
    "            model.\n",
    "        '''\n",
    "        self.sentences = sentences\n",
    "        self.question = question\n",
    "        self.answer = answer\n",
    "        self.hints = hints\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\"Training example: \\n\\t Info: %s \\n\\t Question: %s \\n\\t Answer: %s \\n\\t Hint: %s \\n\"\n",
    "                % (self.sentences, self.question, self.answer, self.hints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class wordVectors(object):\n",
    "    def __init__(self, dataset):\n",
    "        self.words_to_idx, self.idx_to_word = self._map_words_to_idx(dataset)\n",
    "\n",
    "    def _map_words_to_idx(self, dataset):\n",
    "        tokens = []\n",
    "        for example in dataset:\n",
    "            # add all supporting sentence words\n",
    "            for sentence in example.sentences:\n",
    "                tokens += tokenize(sentence)\n",
    "\n",
    "            tokens += tokenize(example.question)\n",
    "            tokens += tokenize(example.answer)\n",
    "\n",
    "        tokens = set(tokens)\n",
    "\n",
    "        # loop over the tokens and establish a canonical word <-> idx mapping\n",
    "        words_to_idx = {}\n",
    "        idx_to_words = {}\n",
    "        counter = 0\n",
    "        for token in tokens:\n",
    "            token = token.lower()\n",
    "            if token not in words_to_idx:\n",
    "                words_to_idx[token] = counter\n",
    "                idx_to_words[counter] = token\n",
    "                counter += 1\n",
    "\n",
    "        return words_to_idx, idx_to_words\n",
    "    \n",
    "    def get_wv_matrix(self, dimension, glove_dir=None):\n",
    "        r = 0.001\n",
    "        self.wv_matrix = np.random.rand(dimension, len(self.words_to_idx)) * 2 * r - r  # TODO: pick initialization carefully\n",
    "        if glove_dir is not None:\n",
    "            pretrained = load_glove_vectors(glove_dir, dimension)\n",
    "\n",
    "            for word in self.words_to_idx:\n",
    "                if word in pretrained:\n",
    "                    self.wv_matrix[:, self.words_to_idx[word]] = pretrained[word].ravel()\n",
    "\n",
    "        return self.wv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def examples_to_example_ind(wordVectors, examples):\n",
    "    outputs = []\n",
    "    for example in examples:\n",
    "        new_sents = []\n",
    "        for sentence in example.sentences:\n",
    "            new_sents.append(np.array([wordVectors.words_to_idx[word] for word in tokenize(sentence)], dtype='int32'))\n",
    "\n",
    "        sentences = np.zeros((len(new_sents), max(len(s) for s in new_sents)), dtype='int32')\n",
    "        mask = np.zeros_like(sentences,  dtype='int32')\n",
    "        for i, sent in enumerate(new_sents):\n",
    "            sentences[i, :len(sent)] = sent\n",
    "            mask[i, :len(sent)] = 1\n",
    "\n",
    "        new_quest = np.array([wordVectors.words_to_idx[word] for word in tokenize(example.question)], dtype='int32')\n",
    "        new_ans = np.array([wordVectors.words_to_idx[word] for word in tokenize(example.answer)], dtype='int32')\n",
    "\n",
    "        new_hints = np.zeros((sentences.shape[0], ), dtype='int32')\n",
    "        new_hints[example.hints] = 1\n",
    "        outputs.append(example_ind(sentences, mask, new_quest, new_ans, new_hints))\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_directions(examples):\n",
    "    directions = {'n': 'north', 'e': 'east', 's': 'south', 'w': 'west'}\n",
    "    for example in examples:\n",
    "        dirs = example.answer.split(',')\n",
    "        newdirs = [directions[d] for d in dirs]\n",
    "        example.answer = \" \".join(newdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_to_examples(file):\n",
    "    f = open(file, \"r\")\n",
    "    lines = f.readlines()\n",
    "    information = []\n",
    "    questans = []\n",
    "\n",
    "    # Want tuples (information, information ..., information, answer)\n",
    "    for line in lines:\n",
    "        split = line.strip().split('\\t')\n",
    "        linesplit = split[0].split(' ')\n",
    "        linenum = int(linesplit[0])\n",
    "        sentence = \" \".join(linesplit[1:]).strip()\n",
    "\n",
    "        # Signals start of new set\n",
    "        if linenum == 1:\n",
    "            information = []\n",
    "            hint_to_arr_idx = {}\n",
    "            diff = 1\n",
    "\n",
    "        # For each question, add as the information all of the previous\n",
    "        # sentences that could have been relevent.\n",
    "        if sentence[-1] == \"?\":\n",
    "            question = sentence\n",
    "            answer = split[1]\n",
    "            hints = map(int, split[2].split(' '))\n",
    "\n",
    "            hint_idxs = [hint_to_arr_idx[i] for i in hints]\n",
    "\n",
    "            questans.append(example(sentences=list(information),\n",
    "                                    answer=answer,\n",
    "                                    question=question,\n",
    "                                    hints=hint_idxs))\n",
    "            diff += 1\n",
    "        else:\n",
    "            information.append(sentence)\n",
    "            hint_to_arr_idx[linenum] = linenum - diff\n",
    "\n",
    "    return questans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_to_relevant_examples(file):\n",
    "    f = open(file, \"r\")\n",
    "    lines = f.readlines()\n",
    "    information = []\n",
    "    questans = []\n",
    "    all_info = []\n",
    "\n",
    "    # Want tuples (information, information ..., information, answer)\n",
    "    for line in lines:\n",
    "        split = line.strip().split('\\t')\n",
    "        linesplit = split[0].split(' ')\n",
    "        linenum = int(linesplit[0])\n",
    "        sentence = \" \".join(linesplit[1:]).strip()\n",
    "\n",
    "        # Signals start of new set\n",
    "        if linenum == 1:\n",
    "            information = []\n",
    "            all_info = []\n",
    "\n",
    "        all_info.append(sentence)\n",
    "        # For each question, add as the information all of the previous\n",
    "        # sentences that could have been relevent.\n",
    "        if sentence[-1] == \"?\":\n",
    "            question = sentence\n",
    "            answer = split[1]\n",
    "            hint = split[2]\n",
    "\n",
    "            relevant = [all_info[i-1] for i in map(int, hint.split(' '))]\n",
    "            questans.append(example(sentences=list(relevant),\n",
    "                                    answer=answer,\n",
    "                                    question=question,\n",
    "                                    hints=hint))\n",
    "        else:\n",
    "            information.append(sentence)\n",
    "\n",
    "    return questans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    return [token.lower() for token in re.findall(r\"[\\w']+|[.,!?;]\", sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_path(datadir, tasknum, test=False):\n",
    "    fnames = {\n",
    "        1: \"single-supporting-fact\",\n",
    "        2: \"two-supporting-facts\",\n",
    "        3: \"three-supporting-facts\",\n",
    "        4: \"two-arg-relations\",\n",
    "        5: \"three-arg-relations\",\n",
    "        6: \"yes-no-questions\",\n",
    "        7: \"counting\",\n",
    "        8: \"lists-sets\",\n",
    "        9: \"simple-negation\",\n",
    "        10: \"indefinite-knowledge\",\n",
    "        11: \"basic-coreference\",\n",
    "        12: \"conjunction\",\n",
    "        13: \"compound-coreference\",\n",
    "        14: \"time-reasoning\",\n",
    "        15: \"basic-deduction\",\n",
    "        16: \"basic-induction\",\n",
    "        17: \"positional-reasoning\",\n",
    "        18: \"size-reasoning\",\n",
    "        19: \"path-finding\",\n",
    "        20: \"agents-motivations\"\n",
    "    }\n",
    "    if(tasknum < 1 or tasknum > 20):\n",
    "        raise NotImplementedError(\"Task %d is not valid\" % tasknum)\n",
    "\n",
    "    traintest = \"test\" if test else \"train\"\n",
    "    fname = (\"qa%d_%s_%s.txt\") % (tasknum, fnames[tasknum], traintest)\n",
    "    return datadir + fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_relevant_data(datadir, tasknum, test=False):\n",
    "    train_examples = file_to_relevant_examples(get_file_path(datadir, tasknum, False))\n",
    "    test_examples = file_to_relevant_examples(get_file_path(datadir, tasknum, True))\n",
    "\n",
    "    if tasknum == 19:\n",
    "        # hack to replace directions with their actual words\n",
    "        fix_directions(train_examples)\n",
    "        fix_directions(test_examples)\n",
    "    if test:\n",
    "        print( 'WARNING: Loading TEST SET')\n",
    "        return train_examples, test_examples\n",
    "    else:\n",
    "        return train_examples, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(datadir, tasknum, test=False):\n",
    "    train_examples = file_to_examples(get_file_path(datadir, tasknum, False))\n",
    "    test_examples = file_to_examples(get_file_path(datadir, tasknum, True))\n",
    "\n",
    "    if tasknum == 19:\n",
    "        # hack to replace directions with their actual words\n",
    "        fix_directions(train_examples)\n",
    "        fix_directions(test_examples)\n",
    "    if test:\n",
    "        return train_examples, test_examples\n",
    "    else:\n",
    "        return train_examples, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_glove_vectors(dimension):\n",
    "    if dimension not in [50, 100, 200, 300]:\n",
    "        raise NotImplementedError('No Glove Vectors with dimension %d' % dimension)\n",
    "    file_name = 'glove.6B.%dd.txt' % dimension\n",
    "    file_path = '/media/ai2-rey/data_disk/data_sets/glove.6B/' + file_name\n",
    "    wvecs = {}\n",
    "    print( 'loading glove vectors')\n",
    "    with open(file_path) as f_glove:\n",
    "        for i, line in enumerate(f_glove):\n",
    "            elems = line.split()\n",
    "            word = elems[0]\n",
    "            vec = np.array([float(x) for x in elems[1:]]).reshape(-1, 1)\n",
    "            wvecs[word] = vec\n",
    "            if i % 20000 == 0:\n",
    "                print( i)\n",
    "    print ('done')\n",
    "    return wvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_design_matrix(dset):\n",
    "    X = np.zeros((len(dset), 2 * n))\n",
    "    for i, ex in enumerate(dset):\n",
    "        for sentence in ex.sentences:\n",
    "            for word in tokenize(sentence):\n",
    "                X[i, words_to_idx[word]] += 1\n",
    "        for word in tokenize(ex.question):\n",
    "            X[i, n + words_to_idx[word]] += 1\n",
    "    y = np.array([words_to_idx[tokenize(ex.answer)[0]] for ex in dset])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Training Accuracy:  0.4285\n",
      "Testing Accuracy:  0.437\n",
      "2\n",
      "Training Accuracy:  0.3501\n",
      "Testing Accuracy:  0.362\n",
      "3\n",
      "Training Accuracy:  0.2995\n",
      "Testing Accuracy:  0.259\n",
      "4\n",
      "Training Accuracy:  0.6946\n",
      "Testing Accuracy:  0.695\n",
      "5\n",
      "Training Accuracy:  0.6416\n",
      "Testing Accuracy:  0.613\n",
      "6\n",
      "Training Accuracy:  0.5146\n",
      "Testing Accuracy:  0.474\n",
      "7\n",
      "Training Accuracy:  0.7175\n",
      "Testing Accuracy:  0.735\n",
      "8\n",
      "Training Accuracy:  0.7042\n",
      "Testing Accuracy:  0.69\n",
      "9\n",
      "Training Accuracy:  0.6278\n",
      "Testing Accuracy:  0.616\n",
      "10\n",
      "Training Accuracy:  0.4773\n",
      "Testing Accuracy:  0.452\n",
      "11\n",
      "Training Accuracy:  0.413\n",
      "Testing Accuracy:  0.407\n",
      "12\n",
      "Training Accuracy:  0.417\n",
      "Testing Accuracy:  0.402\n",
      "13\n",
      "Training Accuracy:  0.4473\n",
      "Testing Accuracy:  0.393\n",
      "14\n",
      "Training Accuracy:  0.502\n",
      "Testing Accuracy:  0.456\n",
      "15\n",
      "Training Accuracy:  0.556\n",
      "Testing Accuracy:  0.57\n",
      "16\n",
      "Training Accuracy:  0.4731\n",
      "Testing Accuracy:  0.489\n",
      "17\n",
      "Training Accuracy:  0.5133\n",
      "Testing Accuracy:  0.511\n",
      "18\n",
      "Training Accuracy:  0.529\n",
      "Testing Accuracy:  0.558\n",
      "19\n",
      "Training Accuracy:  0.2947\n",
      "Testing Accuracy:  0.271\n",
      "20\n",
      "Training Accuracy:  0.9144\n",
      "Testing Accuracy:  0.89\n"
     ]
    }
   ],
   "source": [
    "datadir = '/media/ai2-rey/data_disk/data_sets/bAbI/tasks_1-20_v1-2/en-10k/'\n",
    "clf = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "for task in range(1,21):\n",
    "    train_ex, test_ex = get_data(datadir, task, test=True)\n",
    "    \n",
    "    word_vectors = wordVectors(train_ex)\n",
    "    words_to_idx, idx_to_words = word_vectors.words_to_idx, word_vectors.idx_to_word\n",
    "    n = len(words_to_idx.keys())\n",
    "    \n",
    "    X_train, y_train = get_design_matrix(train_ex)\n",
    "    X_test, y_test = get_design_matrix(test_ex)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    print (task)\n",
    "    print('Training Accuracy: ', clf.score(X_train, y_train))\n",
    "    print('Testing Accuracy: ', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training example: \n",
       "\t Info: ['Sumit is tired.', 'Sumit went back to the bedroom.'] \n",
       "\t Question: Why did sumit go to the bedroom? \n",
       "\t Answer: tired \n",
       "\t Hint: [0] "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ex[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "        1.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "        1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tired'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_words[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_env]",
   "language": "python",
   "name": "conda-env-tensorflow_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
