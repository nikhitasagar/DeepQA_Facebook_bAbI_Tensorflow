{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "import os\n",
    "import gensim\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import operator\n",
    "import math\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.models.rnn.translate import seq2seq_model\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BabiDataset:\n",
    "    def __init__(self, data_dir, task_id, model_type, max_vocab_size=None):\n",
    "        self.task_id = task_id\n",
    "        self.data_dir = data_dir\n",
    "        self.model_type = model_type\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.vocab = set()\n",
    "        self.word_counter = dict()\n",
    "        self.raw_train_data = None\n",
    "        self.raw_test_data = None\n",
    "        self.num_tokens = None\n",
    "        \n",
    "        self.__load_data()\n",
    "        self.word2id_dict, self.id2word_dict = self.__create_word2id_dict()\n",
    "        self.id2vector_dict = self.__create_id2vector_dict()\n",
    "                \n",
    "        (self.train_input_raw, self.train_input_tokens, \n",
    "         self.train_labels_raw, self.train_labels_tokens, self.train_sentence_counts)  = self.__tokenize_sentences(self.raw_train_data)\n",
    "        \n",
    "        (self.test_input_raw, self.test_input_tokens, \n",
    "         self.test_labels_raw, self.test_labels_tokens, self.test_sentence_counts)  = self.__tokenize_sentences(self.raw_test_data)\n",
    "        \n",
    "        self.max_context_len = None\n",
    "        self.max_question_len = None\n",
    "        self.max_answer_len = None\n",
    " \n",
    "        self.pad_sequences()\n",
    "        \n",
    "        #self.train_C_vectors = self.__vectorize_sentences(list(zip(*self.train_input_tokens))[0])\n",
    "        #self.train_Q_vectors = self.__vectorize_sentences(list(zip(*self.train_input_tokens))[1])\n",
    "        #self.train_input_vectors = list(zip(self.train_C_vectors, self.train_Q_vectors))\n",
    "        #self.train_labels_vectors = self.__vectorize_sentences(self.train_labels_tokens)\n",
    "        \n",
    "        #self.test_C_vectors = self.__vectorize_sentences(list(zip(*self.test_input_tokens))[0])\n",
    "        #self.test_Q_vectors = self.__vectorize_sentences(list(zip(*self.test_input_tokens))[1])\n",
    "        #self.test_input_vectors = list(zip(self.train_C_vectors, self.test_Q_vectors))\n",
    "        #self.test_labels_vectors = self.__vectorize_sentences(self.test_labels_tokens)\n",
    "        \n",
    "\n",
    "    def __update_word_counter(self, sequence):\n",
    "        \"\"\" Update word_counter with counts for words in a sentence\n",
    "        \n",
    "        Args:\n",
    "            sequence (list<str>) : list of words in a sequence\n",
    "        \n",
    "        \"\"\"\n",
    "        for word in sequence.split():\n",
    "            self.word_counter[word] = self.word_counter.get(word, 0) + 1\n",
    "            \n",
    "    def __create_vocab(self):\n",
    "        \"\"\" Create set of most frequent unique words found in the training data \"\"\"\n",
    "        \n",
    "        if self.max_vocab_size == None:\n",
    "            self.vocab = set(self.word_counter.keys())\n",
    "        else:\n",
    "            self.vocab = set(sorted(self.word_counter, key=self.word_counter.get, reverse=True)[:self.max_vocab_size])\n",
    "        \n",
    "    def __clean_words(self, line):\n",
    "        punctuation = [x for x in list(string.punctuation)]\n",
    "        space_punct = [' {0}'.format(elem) for elem in punctuation]\n",
    "        replace_punctuation = str.maketrans(dict(zip(punctuation, space_punct)))\n",
    "        line = line.translate(replace_punctuation)\n",
    "        line = line.strip()\n",
    "        line = line.replace('.', ' . ')\n",
    "        line = line.replace(',', ' , ')\n",
    "        line = line[line.find(' ') + 1:]\n",
    "        line = line.lower()\n",
    "        return line\n",
    "    \n",
    "    def __parse_babi_file(self, txt_file):\n",
    "        with open(txt_file) as babi_file:\n",
    "            raw_data = []\n",
    "            curr_sample = None\n",
    "            for i, line in enumerate(open(txt_file)):\n",
    "                id = int(line[0:line.find(' ')])\n",
    "                if id == 1:\n",
    "                    skip = False\n",
    "                    curr_sample = {\"C\": [], \"Q\": \"\", \"A\": \"\"}\n",
    "\n",
    "                line = self.__clean_words(line)\n",
    "                \n",
    "                self.__update_word_counter(line)\n",
    "                if line.find('?') == -1:\n",
    "                    curr_sample[\"C\"].append(line)\n",
    "                else:\n",
    "                    idx = line.find('?')\n",
    "                    tmp = line[idx + 1:].split('\\t')\n",
    "                    curr_sample[\"Q\"] = line[:idx]\n",
    "                    if self.task_id==19:\n",
    "                        #curr_sample[\"A\"] = tmp[1].strip().split(\" , \")\n",
    "                        dirs = tmp[1].strip().split(\"  , \")\n",
    "                        directions = {'n': 'north', 'e': 'east', 's': 'south', 'w': 'west'}\n",
    "                        newdirs = [directions[d] for d in dirs]\n",
    "                        curr_sample[\"A\"] = \" \".join(newdirs)\n",
    "                            \n",
    "                    else:\n",
    "                        curr_sample[\"A\"] = tmp[1].strip()\n",
    "                    raw_data.append(deepcopy(curr_sample))\n",
    "\n",
    "            self.__create_vocab()\n",
    "            print(\"Loaded {} data samples from {}\".format(len(raw_data), txt_file.split(self.data_dir)[1]))\n",
    "            return raw_data\n",
    "\n",
    "    def __load_data(self):\n",
    "        files = [os.path.join(self.data_dir, f) for f in os.listdir(self.data_dir)]\n",
    "        s = 'qa{}_'.format(self.task_id)\n",
    "        task_files = [f for f in files if s in f]\n",
    "        train_file = [f for f in task_files if 'train' in f][0]\n",
    "        test_file = [f for f in task_files if 'test' in f][0] \n",
    "        \n",
    "        self.raw_train_data = self.__parse_babi_file(train_file)\n",
    "        self.raw_test_data = self.__parse_babi_file(test_file)\n",
    "        \n",
    "    \n",
    "    def __create_word2id_dict(self):\n",
    "        word2id_dict = dict()\n",
    "        \n",
    "        word2id_dict['PAD'] = 0\n",
    "        word2id_dict['UNK'] = 1\n",
    "        word2id_dict['Q'] = 2\n",
    "            \n",
    "        for word in self.vocab:\n",
    "            word2id_dict[word] = len(word2id_dict)\n",
    "        \n",
    "        self.vocab.add('PAD')\n",
    "        self.vocab.add('UNK')\n",
    "        self.vocab.add('Q')\n",
    "        \n",
    "        id2word_dict = dict(list(zip(word2id_dict.values(), word2id_dict.keys())))\n",
    "        self.num_tokens = len(word2id_dict)\n",
    "        return word2id_dict, id2word_dict\n",
    "    \n",
    "    def __convert_word2id(self, word):\n",
    "        try:\n",
    "            word_id = self.word2id_dict[word]\n",
    "        except:\n",
    "            word_id = self.word2id_dict['UNK']\n",
    "        return word_id\n",
    "    \n",
    "    def __create_id2vector_dict(self):\n",
    "        embeddings_file = '/media/ai2-rey/data_disk/data_sets/glove.6B/glove.6B.100d.txt'\n",
    "        \n",
    "        glove_embeddings = {}\n",
    "\n",
    "        with open(embeddings_file) as read_file:\n",
    "            for line in read_file:\n",
    "                embedding = line.split('\\n')[0].split(' ')\n",
    "                embedding_key = embedding[0]\n",
    "                embedding_vec = np.array([float(val) for val in embedding[1:]])\n",
    "                glove_embeddings.update({embedding_key : embedding_vec})\n",
    "\n",
    "        id2vector_dict = {}        \n",
    "        for word, word_id in self.word2id_dict.items():\n",
    "            if word in glove_embeddings:\n",
    "                id2vector_dict[word_id] = glove_embeddings[word]\n",
    "            else: \n",
    "                id2vector_dict[word_id] = np.random.uniform(0.0,1.0,100)        \n",
    "        return id2vector_dict    \n",
    "\n",
    "    def __vectorize_sentences(self, sequences):\n",
    "        vectors = []\n",
    "        \n",
    "        for sequence in sequences:\n",
    "            sentence_vectors = []\n",
    "            for word_id in sequence:\n",
    "                sentence_vectors.append(self.id2vector_dict[word_id])\n",
    "            vectors.append(sentence_vectors)\n",
    "        return vectors\n",
    "\n",
    "    \n",
    "    def __convert_to_one_hot(self, labels):\n",
    "        one_hot = np.array([[0 for j in range(self.num_tokens)] for i in range(len(labels))])\n",
    "\n",
    "        for i in range(len(one_hot)):\n",
    "            one_hot[i][labels[i]] = 1\n",
    "\n",
    "        return one_hot\n",
    "    \n",
    "    def __tokenize_sentences(self, raw_data):\n",
    "        \"\"\" Tokenizes sentences.\n",
    "        :param raw: dict returned from load_babi\n",
    "        :param word_table: WordTable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        context = []\n",
    "        context_ids = []\n",
    "        questions = []\n",
    "        question_ids = []\n",
    "        answers = []\n",
    "        answer_ids = []\n",
    "        sentence_counts = []\n",
    "\n",
    "        for sample in raw_data:\n",
    "            story = []\n",
    "            story_ids = []\n",
    "            for sentence in sample[\"C\"]:\n",
    "                seq = [w for w in sentence.lower().split(' ') if len(w) > 0]\n",
    "                seq_ids = [self.__convert_word2id(w) for w in sentence.lower().split(' ') if len(w) > 0]\n",
    "                story.append(seq)\n",
    "                story_ids.append(seq_ids)\n",
    "\n",
    "            q = [w for w in sample[\"Q\"].lower().split(' ') if len(w) > 0]\n",
    "            q_ids = [self.__convert_word2id(w) for w in sample[\"Q\"].lower().split(' ') if len(w) > 0]\n",
    "\n",
    "            context.append([word for sentence in story for word in sentence] + ['Q'] + q)\n",
    "            context_ids.append([word_id for sentence in story_ids for word_id in sentence] + [self.word2id_dict['Q']] + q_ids)\n",
    "\n",
    "#                 answers.append(['GO'] + sample[\"A\"].lower().split(' ') + ['EOS'])\n",
    "            answers.append([w for w in sample[\"A\"].lower().split(' ') if len(w)>0])  # NOTE: here we assume the answer is one word!\n",
    "            answer_ids.append([self.__convert_word2id(w) for w in sample[\"A\"].lower().split(' ') if len(w)>0])\n",
    "\n",
    "            sentence_counts.append(len(story))    \n",
    "        context_ids, answer_ids = np.array(context_ids), np.array(answer_ids)\n",
    "        packaged_data = list(zip(context, context_ids, answers, answer_ids, sentence_counts))\n",
    "        random.shuffle(packaged_data)\n",
    "        context, context_ids, answers, answer_ids, sentence_counts = list(zip(*packaged_data))\n",
    "\n",
    "        answer_ids = list(answer_ids)\n",
    "        if self.task_id !=8:\n",
    "            for i, a_id in enumerate(answer_ids):                   \n",
    "                answer_ids[i] = a_id.tolist()\n",
    "        return context, context_ids, answers, answer_ids, sentence_counts\n",
    "            \n",
    "    def __get_max_sequence_length(self, sequences):\n",
    "        max_len = 0\n",
    "        min_len = 1000\n",
    "        avg_len = 0\n",
    "        for sequence in sequences:\n",
    "            max_len = max(max_len, len(sequence))\n",
    "            min_len = min(min_len, len(sequence))\n",
    "            avg_len += len(sequence)\n",
    "        avg_len = int(float(avg_len) / len(sequences))\n",
    "        return max_len, min_len, avg_len\n",
    "    \n",
    "    def __apply_padding(self, sequences, length):\n",
    "        padded_data = []\n",
    "        for id_sequence in sequences:\n",
    "            if len(id_sequence) < length:\n",
    "                padded_sequence = id_sequence\n",
    "                for i in range(length - len(id_sequence)):\n",
    "                    padded_sequence.append(0)\n",
    "                padded_data.append(np.array(padded_sequence)) \n",
    "            elif len(id_sequence) > length:\n",
    "                clipped_sequence = id_sequence[:length]\n",
    "                padded_data.append(np.array(clipped_sequence))\n",
    "            else:\n",
    "                padded_data.append(np.array(id_sequence))\n",
    "\n",
    "        return np.array(padded_data)\n",
    "\n",
    "    def pad_sequences(self, pad_lengths=None):\n",
    "                \n",
    "        train_context_data = self.train_input_tokens\n",
    "        train_answer_data = self.train_labels_tokens\n",
    "\n",
    "        test_context_data = self.test_input_tokens\n",
    "        test_answer_data = self.test_labels_tokens\n",
    "\n",
    "        if pad_lengths == None:\n",
    "            self.max_context_len, min_context_len, avg_context_len = self.__get_max_sequence_length(train_context_data)\n",
    "            print('Context Lengths: max = {}, min = {}, avg = {}'.format(self.max_context_len, min_context_len, avg_context_len))\n",
    "\n",
    "            self.max_answer_len, min_answer_len, avg_answer_len = self.__get_max_sequence_length(train_answer_data)\n",
    "            print('Answer Lengths: max = {}, min = {}, avg = {}'.format(self.max_answer_len, min_answer_len, avg_answer_len))\n",
    "        elif len(pad_lengths) == 2:\n",
    "            self.max_context_len = pad_lengths[0]\n",
    "            self.max_question_len = pad_lengths[1]\n",
    "        else:\n",
    "            print(\"Error: pad_lengths needs form [context_len, question_len]\")\n",
    "\n",
    "        train_context_data = self.__apply_padding(train_context_data, self.max_context_len)\n",
    "        train_answer_data = self.__apply_padding(train_answer_data, self.max_answer_len)\n",
    "\n",
    "        test_context_data = self.__apply_padding(test_context_data, self.max_context_len)\n",
    "        test_answer_data = self.__apply_padding(test_answer_data, self.max_answer_len)\n",
    "\n",
    "        self.train_input_tokens = train_context_data\n",
    "        self.train_labels_tokens = train_answer_data \n",
    "\n",
    "        self.test_input_tokens = test_context_data\n",
    "        self.test_labels_tokens = test_answer_data\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2Seq:\n",
    " \n",
    "    def __init__(self, vocab_size, xseq_len, yseq_len, num_layers, lr_rate=0.001, \n",
    "                 momentum = 0.9, n_hidden=256, word_dim=100, \n",
    "                 dropout_rate=1., gpu_device=0, model_dir=None):\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.xseq_len = xseq_len\n",
    "        self.yseq_len = yseq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.lr_rate = lr_rate\n",
    "        self.momentum = momentum\n",
    "        self.n_hidden = n_hidden\n",
    "        self.word_dim = word_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.gpu_device = gpu_device\n",
    "        self.model_dir = self.__prepare_model_dir(model_dir)\n",
    "        self.__keep_prob = None\n",
    "        \n",
    "        self.__graph = tf.Graph()\n",
    "        \n",
    "        self.__build_model()\n",
    "    \n",
    "    def __prepare_model_dir(self, model_dir):\n",
    "        \"\"\" Checks model directory for a weights folder and creates one if none exists\n",
    "        \n",
    "        Args:\n",
    "            model_dir (str) : defines directory location to save weights and training log file\n",
    "            \n",
    "        Returns:\n",
    "            str : directory location with weights folder\n",
    "        \n",
    "        \"\"\"\n",
    "        if model_dir == None:\n",
    "            model_dir = os.getcwd() + '/'\n",
    "        else:\n",
    "            if model_dir[-1] != '/':\n",
    "                model_dir = model_dir + '/'\n",
    "            else:\n",
    "                model_dir = model_dir\n",
    "        \n",
    "        if not os.path.exists(model_dir + 'weights'):\n",
    "            os.makedirs(model_dir + 'weights')\n",
    "        return model_dir\n",
    "                   \n",
    "    def __build_model(self):\n",
    "        \"\"\" Creates computation graph for dual encoder LSTM. Includes structure for training and deploying. \"\"\"\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        gpu_device_name = '/gpu:{}'.format(self.gpu_device)\n",
    "        \n",
    "        with self.__graph.as_default():\n",
    "            with tf.device(gpu_device_name):\n",
    "                # define placeholder variables for model inputs\n",
    "                self.enc_inp = [tf.placeholder(shape=[None,], \n",
    "                                               dtype=tf.int32, \n",
    "                                               name='ei_{}'.format(t)) for t in range(self.xseq_len)]\n",
    "                self.labels = [tf.placeholder(shape=[None,], \n",
    "                                               dtype=tf.int32, \n",
    "                                               name='ei_{}'.format(t)) for t in range(self.yseq_len)]\n",
    "                self.dec_inp = ([tf.zeros_like(self.enc_inp[0], dtype=np.int32, name=\"GO\")]+self.labels[:-1]) \n",
    "                \n",
    "                self.__keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "                # Build the RNN\n",
    "                with tf.variable_scope(\"decoder\"):\n",
    "                    # We use an LSTM Cell\n",
    "                    #cell = tf.nn.rnn_cell.LSTMCell(self.n_hidden, forget_bias=2.0, state_is_tuple=True)\n",
    "                    cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                           tf.nn.rnn_cell.BasicLSTMCell(word_dim, state_is_tuple=True),\n",
    "                           output_keep_prob=self.__keep_prob)\n",
    "                    \n",
    "                    stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([cell]*self.num_layers, state_is_tuple=True)\n",
    "                    \n",
    "                    self.dec_outputs, self.dec_states = tf.nn.seq2seq.embedding_rnn_seq2seq(\n",
    "                                                        self.enc_inp, self.dec_inp, stacked_lstm, \n",
    "                                                        vocab_size, vocab_size, word_dim)\n",
    "                    \n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                    self.dec_outputs_test, self.dec_states_test = tf.nn.seq2seq.embedding_rnn_seq2seq(\n",
    "                                                        self.enc_inp, self.dec_inp, stacked_lstm, \n",
    "                                                        vocab_size, vocab_size, word_dim,\n",
    "                                                        feed_previous=True) \n",
    "                    \n",
    "                loss_weights = [tf.ones_like(l, dtype=tf.float32) for l in self.labels]\n",
    "\n",
    "                self.__loss = tf.nn.seq2seq.sequence_loss(self.dec_outputs, self.labels, loss_weights, vocab_size)\n",
    "\n",
    "                self.__optimizer = tf.train.AdamOptimizer(learning_rate=self.lr_rate).minimize(self.__loss)\n",
    "\n",
    "                self.__init = tf.global_variables_initializer()\n",
    "    \n",
    "    def get_feed(self,X,Y, keep_prob):\n",
    "        feed_dict={self.enc_inp[t]: X[t] for t in range(self.xseq_len)}\n",
    "        feed_dict.update({self.labels[t]: Y[t] for t in range(self.yseq_len)})\n",
    "        feed_dict[self.__keep_prob]=keep_prob\n",
    "        return feed_dict\n",
    "\n",
    "    def train_batch(self,sess, data_iter):\n",
    "        X,Y = data_iter.next_batch()\n",
    "        feed_dict = self.get_feed(X,Y, self.dropout_rate)\n",
    "        _, loss_v = sess.run([self.__optimizer, self.__loss], feed_dict)\n",
    "        return loss_v\n",
    "     \n",
    "    def train(self, train_data_iter, test_data_iter, deploy_data_iter,\n",
    "              deploy_interval = 1000, train_iters=10000, display_step=200, \n",
    "              save_weights_interval=5000, id2word_dict=None, weights_prefix=None):\n",
    "\n",
    "        \n",
    "        with tf.Session(graph=self.__graph, config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "            sess.run(self.__init)\n",
    "            saver = tf.train.Saver(max_to_keep=100)\n",
    "            # Keep training until reach max iterations\n",
    "            for train_iter in range(train_iters):\n",
    "                train_iter += 1\n",
    "                \n",
    "                loss = self.train_batch(sess, train_data_iter)\n",
    "                if train_iter % display_step == 0:\n",
    "\n",
    "                    train_loss_string = \"Iter {}, Minibatch Loss = {:.6f}\".format(train_iter, loss)\n",
    "                    print(train_loss_string)\n",
    "                \n",
    "                if train_iter % save_weights_interval ==0:\n",
    "                    if weights_prefix != None:\n",
    "                        weights_dir = self.model_dir + \"weights/{}_iter-{}.cpkt\".format(weights_prefix,train_iter)\n",
    "                    else:\n",
    "                        weights_dir = self.model_dir + \"weights/QA_seq2seq_weights_iter-{}.ckpt\".format(train_iter)\n",
    "                    save_path = saver.save(sess, weights_dir)\n",
    "                    save_string = \"Model saved in file: {}\".format(save_path)\n",
    "                    print(save_string)\n",
    "                    test_loss, test_accuracy = self.test(sess, test_data_iter, 1)\n",
    "                    print('Test loss @ iter {}: {} '.format(train_iter, test_loss))\n",
    "                    print('Test Accuracy @ iter {}: {} '.format(train_iter, test_accuracy))\n",
    "                \n",
    "#                 if deploy_data_iter !=None and train_iter % deploy_interval ==0:\n",
    "#                     d_X, d_Y = deploy_data_iter.next_batch()\n",
    "#                     d_feed_dict = self.get_feed(d_X,d_Y, 1.)\n",
    "                                        \n",
    "                    \n",
    "    def test_step(self, test_data_iter, sess):\n",
    "        testX, testY = test_data_iter.next_batch()\n",
    "        feed_dict = self.get_feed(testX, testY, keep_prob=1.)\n",
    "        loss_v, dec_op_v = sess.run([self.__loss, self.dec_outputs_test], feed_dict)\n",
    "        dec_op_v = np.array(dec_op_v).transpose([1,0,2])\n",
    "        return loss_v, dec_op_v, testX, testY\n",
    "    \n",
    "    def test(self, sess, test_data_iter, num_batches):\n",
    "        losses= []\n",
    "        predict_loss = []\n",
    "        for i in range(num_batches):\n",
    "            loss_t, dec_op_t, batchX, batchY = self.test_step(test_data_iter, sess)\n",
    "            losses.append(loss_t)\n",
    "            \n",
    "            for idx in range(len(dec_op_t)):\n",
    "                real = batchY.T[idx]\n",
    "                predict = np.argmax(dec_op_t, axis=2)[idx]\n",
    "                predict_loss.append(all(real==predict))\n",
    "        return np.mean(losses), np.mean(predict_loss)        \n",
    "    \n",
    "    def predict(self, ckpt_file, X):\n",
    "        with tf.Session(graph=self.__graph, config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "            sess.run(self.__init)\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, self.model_dir + 'weights/' + ckpt_file)\n",
    "            feed_dict = {self.enc_inp[t]: X[t] for t in range(self.xseq_len)}\n",
    "            feed_dict[self.__keep_prob] = 1.\n",
    "            dec_op_v = sess.run(self.dec_outputs_test, feed_dict)\n",
    "            # dec_op_v is a list; also need to transpose 0,1 indices \n",
    "            #  (interchange batch_size and timesteps dimensions\n",
    "            dec_op_v = np.array(dec_op_v).transpose([1,0,2])\n",
    "            # return the index of item with highest probability\n",
    "            return np.argmax(dec_op_v, axis=2) \n",
    "        \n",
    "    def test_try(self, ckpt_file, test_data_iter):\n",
    "        with tf.Session(graph=self.__graph, config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "            sess.run(self.__init)\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, self.model_dir + 'weights/' + ckpt_file)\n",
    "            accuracy = []\n",
    "            loss_t, out_t, X, Y = self.test_step(test_data_iter, sess)\n",
    "            \n",
    "            for idx in range(len(out_t)):\n",
    "                real = Y.T[idx]\n",
    "                prediction = np.argmax(out_t, axis=2)[idx]\n",
    "                accuracy.append(all(real==prediction))\n",
    "            return np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataIterator:\n",
    "    def __init__(self, data, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.data_iterator = self.make_random_iter()\n",
    "        \n",
    "    def next_batch(self):\n",
    "        try:\n",
    "            idxs = next(self.data_iterator)\n",
    "        except StopIteration:\n",
    "            self.data_iterator = self.make_random_iter()\n",
    "            idxs = next(self.data_iterator)\n",
    "        X, Y = list(zip(*[self.data[i] for i in idxs]))\n",
    "        X = np.array(X).T\n",
    "        Y = np.array(Y).T\n",
    "        return X, Y\n",
    "\n",
    "    def make_random_iter(self):\n",
    "        splits = np.arange(self.batch_size, len(self.data), self.batch_size)\n",
    "        it = np.split(np.random.permutation(range(len(self.data))), splits)[:-1]\n",
    "        return iter(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task  1\n",
      "Loaded 10000 data samples from qa1_single-supporting-fact_train.txt\n",
      "Loaded 1000 data samples from qa1_single-supporting-fact_test.txt\n",
      "Context Lengths: max = 72, min = 16, avg = 41\n",
      "Answer Lengths: max = 1, min = 1, avg = 1\n",
      "Iter 200, Minibatch Loss = 1.834020\n",
      "Iter 400, Minibatch Loss = 1.822145\n",
      "Iter 600, Minibatch Loss = 1.799796\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_1_iter-600.cpkt\n",
      "Test loss @ iter 600: 1.7887059450149536 \n",
      "Test Accuracy @ iter 600: 0.140625 \n",
      "Iter 800, Minibatch Loss = 1.800823\n",
      "Iter 1000, Minibatch Loss = 1.787349\n",
      "Iter 1200, Minibatch Loss = 1.794114\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_1_iter-1200.cpkt\n",
      "Test loss @ iter 1200: 1.7969083786010742 \n",
      "Test Accuracy @ iter 1200: 0.1796875 \n",
      "Iter 1400, Minibatch Loss = 1.560320\n",
      "Iter 1600, Minibatch Loss = 1.522709\n",
      "Iter 1800, Minibatch Loss = 1.474202\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_1_iter-1800.cpkt\n",
      "Test loss @ iter 1800: 1.458622932434082 \n",
      "Test Accuracy @ iter 1800: 0.3984375 \n",
      "Iter 2000, Minibatch Loss = 1.392055\n",
      "Iter 2200, Minibatch Loss = 1.296082\n",
      "Iter 2400, Minibatch Loss = 1.075151\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_1_iter-2400.cpkt\n",
      "Test loss @ iter 2400: 1.1532787084579468 \n",
      "Test Accuracy @ iter 2400: 0.515625 \n",
      "Iter 2600, Minibatch Loss = 0.680462\n",
      "Iter 2800, Minibatch Loss = 0.353761\n",
      "Iter 3000, Minibatch Loss = 0.169330\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_1_iter-3000.cpkt\n",
      "Test loss @ iter 3000: 0.25911974906921387 \n",
      "Test Accuracy @ iter 3000: 0.890625 \n",
      "Iter 3200, Minibatch Loss = 0.111414\n",
      "Iter 3400, Minibatch Loss = 0.116022\n",
      "Iter 3600, Minibatch Loss = 0.090343\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_1_iter-3600.cpkt\n",
      "Test loss @ iter 3600: 0.15210217237472534 \n",
      "Test Accuracy @ iter 3600: 0.9453125 \n",
      "Iter 3800, Minibatch Loss = 0.099855\n",
      "Iter 4000, Minibatch Loss = 0.075914\n",
      "Iter 4200, Minibatch Loss = 0.046167\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_1_iter-4200.cpkt\n",
      "Test loss @ iter 4200: 0.10087300837039948 \n",
      "Test Accuracy @ iter 4200: 0.9453125 \n",
      "Iter 4400, Minibatch Loss = 0.052571\n",
      "Iter 4600, Minibatch Loss = 0.010272\n",
      "Iter 4800, Minibatch Loss = 0.042651\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_1_iter-4800.cpkt\n",
      "Test loss @ iter 4800: 0.601913332939148 \n",
      "Test Accuracy @ iter 4800: 0.875 \n",
      "Iter 5000, Minibatch Loss = 0.056599\n",
      "Iter 5200, Minibatch Loss = 0.014474\n",
      "Iter 5400, Minibatch Loss = 0.003816\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_1_iter-5400.cpkt\n",
      "Test loss @ iter 5400: 0.08932473510503769 \n",
      "Test Accuracy @ iter 5400: 0.9765625 \n",
      "Iter 5600, Minibatch Loss = 0.010569\n",
      "Iter 5800, Minibatch Loss = 0.004808\n",
      "Iter 6000, Minibatch Loss = 0.004608\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_1_iter-6000.cpkt\n",
      "Test loss @ iter 6000: 0.25858157873153687 \n",
      "Test Accuracy @ iter 6000: 0.9609375 \n",
      "task  2\n",
      "Loaded 10000 data samples from qa2_two-supporting-facts_train.txt\n",
      "Loaded 1000 data samples from qa2_two-supporting-facts_test.txt\n",
      "Context Lengths: max = 427, min = 17, avg = 100\n",
      "Answer Lengths: max = 1, min = 1, avg = 1\n",
      "Iter 200, Minibatch Loss = 1.873097\n",
      "Iter 400, Minibatch Loss = 1.806492\n",
      "Iter 600, Minibatch Loss = 1.798477\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_2_iter-600.cpkt\n",
      "Test loss @ iter 600: 1.7967991828918457 \n",
      "Test Accuracy @ iter 600: 0.15625 \n",
      "Iter 800, Minibatch Loss = 1.797750\n",
      "Iter 1000, Minibatch Loss = 1.796797\n",
      "Iter 1200, Minibatch Loss = 1.789811\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_2_iter-1200.cpkt\n",
      "Test loss @ iter 1200: 1.793204426765442 \n",
      "Test Accuracy @ iter 1200: 0.1640625 \n",
      "Iter 1400, Minibatch Loss = 1.801723\n",
      "Iter 1600, Minibatch Loss = 1.790271\n",
      "Iter 1800, Minibatch Loss = 1.798000\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_2_iter-1800.cpkt\n",
      "Test loss @ iter 1800: 1.789832353591919 \n",
      "Test Accuracy @ iter 1800: 0.1953125 \n",
      "Iter 2000, Minibatch Loss = 1.798223\n",
      "Iter 2200, Minibatch Loss = 1.791968\n",
      "Iter 2400, Minibatch Loss = 1.781573\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_2_iter-2400.cpkt\n",
      "Test loss @ iter 2400: 1.7861132621765137 \n",
      "Test Accuracy @ iter 2400: 0.171875 \n",
      "Iter 2600, Minibatch Loss = 1.795164\n",
      "Iter 2800, Minibatch Loss = 1.794919\n",
      "Iter 3000, Minibatch Loss = 1.796662\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_2_iter-3000.cpkt\n",
      "Test loss @ iter 3000: 1.7837059497833252 \n",
      "Test Accuracy @ iter 3000: 0.1875 \n",
      "Iter 3200, Minibatch Loss = 1.791803\n",
      "Iter 3400, Minibatch Loss = 1.790613\n",
      "Iter 3600, Minibatch Loss = 1.787512\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_2_iter-3600.cpkt\n",
      "Test loss @ iter 3600: 1.7944579124450684 \n",
      "Test Accuracy @ iter 3600: 0.1953125 \n",
      "Iter 3800, Minibatch Loss = 1.801569\n",
      "Iter 4000, Minibatch Loss = 1.796658\n",
      "Iter 4200, Minibatch Loss = 1.790184\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_2_iter-4200.cpkt\n",
      "Test loss @ iter 4200: 1.798448920249939 \n",
      "Test Accuracy @ iter 4200: 0.140625 \n",
      "Iter 4400, Minibatch Loss = 1.782975\n",
      "Iter 4600, Minibatch Loss = 1.795762\n",
      "Iter 4800, Minibatch Loss = 1.791326\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_2_iter-4800.cpkt\n",
      "Test loss @ iter 4800: 1.794405460357666 \n",
      "Test Accuracy @ iter 4800: 0.125 \n",
      "Iter 5000, Minibatch Loss = 1.794824\n",
      "Iter 5200, Minibatch Loss = 1.798181\n",
      "Iter 5400, Minibatch Loss = 1.789510\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_2_iter-5400.cpkt\n",
      "Test loss @ iter 5400: 1.7944045066833496 \n",
      "Test Accuracy @ iter 5400: 0.15625 \n",
      "Iter 5600, Minibatch Loss = 1.791621\n",
      "Iter 5800, Minibatch Loss = 1.794474\n",
      "Iter 6000, Minibatch Loss = 1.798118\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_2_iter-6000.cpkt\n",
      "Test loss @ iter 6000: 1.7934441566467285 \n",
      "Test Accuracy @ iter 6000: 0.171875 \n",
      "task  3\n",
      "Loaded 10000 data samples from qa3_three-supporting-facts_train.txt\n",
      "Loaded 1000 data samples from qa3_three-supporting-facts_test.txt\n",
      "Context Lengths: max = 1883, min = 30, avg = 315\n",
      "Answer Lengths: max = 1, min = 1, avg = 1\n",
      "Iter 200, Minibatch Loss = 1.884580\n",
      "Iter 400, Minibatch Loss = 1.806664\n",
      "Iter 600, Minibatch Loss = 1.793341\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_3_iter-600.cpkt\n",
      "Test loss @ iter 600: 1.7813680171966553 \n",
      "Test Accuracy @ iter 600: 0.296875 \n",
      "Iter 800, Minibatch Loss = 1.795869\n",
      "Iter 1000, Minibatch Loss = 1.790355\n",
      "Iter 1200, Minibatch Loss = 1.795019\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_3_iter-1200.cpkt\n",
      "Test loss @ iter 1200: 1.7892546653747559 \n",
      "Test Accuracy @ iter 1200: 0.140625 \n",
      "Iter 1400, Minibatch Loss = 1.787255\n",
      "Iter 1600, Minibatch Loss = 1.798764\n",
      "Iter 1800, Minibatch Loss = 1.797799\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_3_iter-1800.cpkt\n",
      "Test loss @ iter 1800: 1.7908005714416504 \n",
      "Test Accuracy @ iter 1800: 0.1640625 \n",
      "Iter 2000, Minibatch Loss = 1.794538\n",
      "Iter 2200, Minibatch Loss = 1.796872\n",
      "Iter 2400, Minibatch Loss = 1.797897\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_3_iter-2400.cpkt\n",
      "Test loss @ iter 2400: 1.7931065559387207 \n",
      "Test Accuracy @ iter 2400: 0.1328125 \n",
      "Iter 2600, Minibatch Loss = 1.792804\n",
      "Iter 2800, Minibatch Loss = 1.804919\n",
      "Iter 3000, Minibatch Loss = 1.791731\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_3_iter-3000.cpkt\n",
      "Test loss @ iter 3000: 1.7902520895004272 \n",
      "Test Accuracy @ iter 3000: 0.1484375 \n",
      "Iter 3200, Minibatch Loss = 1.803225\n",
      "Iter 3400, Minibatch Loss = 1.794924\n",
      "Iter 3600, Minibatch Loss = 1.802023\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_3_iter-3600.cpkt\n",
      "Test loss @ iter 3600: 1.7842011451721191 \n",
      "Test Accuracy @ iter 3600: 0.203125 \n",
      "Iter 3800, Minibatch Loss = 1.799550\n",
      "Iter 4000, Minibatch Loss = 1.794005\n",
      "Iter 4200, Minibatch Loss = 1.793004\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_3_iter-4200.cpkt\n",
      "Test loss @ iter 4200: 1.7889859676361084 \n",
      "Test Accuracy @ iter 4200: 0.203125 \n",
      "Iter 4400, Minibatch Loss = 1.802441\n",
      "Iter 4600, Minibatch Loss = 1.793090\n",
      "Iter 4800, Minibatch Loss = 1.793745\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_3_iter-4800.cpkt\n",
      "Test loss @ iter 4800: 1.7852272987365723 \n",
      "Test Accuracy @ iter 4800: 0.203125 \n",
      "Iter 5000, Minibatch Loss = 1.792102\n",
      "Iter 5200, Minibatch Loss = 1.800644\n",
      "Iter 5400, Minibatch Loss = 1.790427\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_3_iter-5400.cpkt\n",
      "Test loss @ iter 5400: 1.7895983457565308 \n",
      "Test Accuracy @ iter 5400: 0.1796875 \n",
      "Iter 5600, Minibatch Loss = 1.796460\n",
      "Iter 5800, Minibatch Loss = 1.790436\n",
      "Iter 6000, Minibatch Loss = 1.787056\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_3_iter-6000.cpkt\n",
      "Test loss @ iter 6000: 1.7904648780822754 \n",
      "Test Accuracy @ iter 6000: 0.171875 \n",
      "task  4\n",
      "Loaded 10000 data samples from qa4_two-arg-relations_train.txt\n",
      "Loaded 1000 data samples from qa4_two-arg-relations_test.txt\n",
      "Context Lengths: max = 23, min = 23, avg = 23\n",
      "Answer Lengths: max = 1, min = 1, avg = 1\n",
      "Iter 200, Minibatch Loss = 0.802937\n",
      "Iter 400, Minibatch Loss = 0.458737\n",
      "Iter 600, Minibatch Loss = 0.425662\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_4_iter-600.cpkt\n",
      "Test loss @ iter 600: 0.3302282392978668 \n",
      "Test Accuracy @ iter 600: 0.8125 \n",
      "Iter 800, Minibatch Loss = 0.385232\n",
      "Iter 1000, Minibatch Loss = 0.194401\n",
      "Iter 1200, Minibatch Loss = 0.022641\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_4_iter-1200.cpkt\n",
      "Test loss @ iter 1200: 0.0037632505409419537 \n",
      "Test Accuracy @ iter 1200: 1.0 \n",
      "Iter 1400, Minibatch Loss = 0.008058\n",
      "Iter 1600, Minibatch Loss = 0.004552\n",
      "Iter 1800, Minibatch Loss = 0.004893\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_4_iter-1800.cpkt\n",
      "Test loss @ iter 1800: 0.00037442511529661715 \n",
      "Test Accuracy @ iter 1800: 1.0 \n",
      "Iter 2000, Minibatch Loss = 0.002776\n",
      "Iter 2200, Minibatch Loss = 0.003861\n",
      "Iter 2400, Minibatch Loss = 0.000999\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_4_iter-2400.cpkt\n",
      "Test loss @ iter 2400: 6.377772660925984e-05 \n",
      "Test Accuracy @ iter 2400: 1.0 \n",
      "Iter 2600, Minibatch Loss = 0.000703\n",
      "Iter 2800, Minibatch Loss = 0.003227\n",
      "Iter 3000, Minibatch Loss = 0.001819\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_4_iter-3000.cpkt\n",
      "Test loss @ iter 3000: 0.00010306484182365239 \n",
      "Test Accuracy @ iter 3000: 1.0 \n",
      "Iter 3200, Minibatch Loss = 0.002055\n",
      "Iter 3400, Minibatch Loss = 0.000737\n",
      "Iter 3600, Minibatch Loss = 0.000615\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_4_iter-3600.cpkt\n",
      "Test loss @ iter 3600: 2.6162113499594852e-05 \n",
      "Test Accuracy @ iter 3600: 1.0 \n",
      "Iter 3800, Minibatch Loss = 0.000216\n",
      "Iter 4000, Minibatch Loss = 0.000240\n",
      "Iter 4200, Minibatch Loss = 0.001465\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_4_iter-4200.cpkt\n",
      "Test loss @ iter 4200: 0.00012659741332754493 \n",
      "Test Accuracy @ iter 4200: 1.0 \n",
      "Iter 4400, Minibatch Loss = 0.000624\n",
      "Iter 4600, Minibatch Loss = 0.001030\n",
      "Iter 4800, Minibatch Loss = 0.000321\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_4_iter-4800.cpkt\n",
      "Test loss @ iter 4800: 1.0661670785339084e-05 \n",
      "Test Accuracy @ iter 4800: 1.0 \n",
      "Iter 5000, Minibatch Loss = 0.000155\n",
      "Iter 5200, Minibatch Loss = 0.000186\n",
      "Iter 5400, Minibatch Loss = 0.000131\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_4_iter-5400.cpkt\n",
      "Test loss @ iter 5400: 5.620506271952763e-06 \n",
      "Test Accuracy @ iter 5400: 1.0 \n",
      "Iter 5600, Minibatch Loss = 0.000129\n",
      "Iter 5800, Minibatch Loss = 0.000325\n",
      "Iter 6000, Minibatch Loss = 0.000162\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_4_iter-6000.cpkt\n",
      "Test loss @ iter 6000: 2.9029242796241306e-06 \n",
      "Test Accuracy @ iter 6000: 1.0 \n",
      "task  5\n",
      "Loaded 10000 data samples from qa5_three-arg-relations_train.txt\n",
      "Loaded 1000 data samples from qa5_three-arg-relations_test.txt\n",
      "Context Lengths: max = 789, min = 18, avg = 131\n",
      "Answer Lengths: max = 1, min = 1, avg = 1\n",
      "Iter 200, Minibatch Loss = 1.980096\n",
      "Iter 400, Minibatch Loss = 1.931977\n",
      "Iter 600, Minibatch Loss = 1.934675\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_5_iter-600.cpkt\n",
      "Test loss @ iter 600: 1.901989221572876 \n",
      "Test Accuracy @ iter 600: 0.234375 \n",
      "Iter 800, Minibatch Loss = 1.910801\n",
      "Iter 1000, Minibatch Loss = 1.896753\n",
      "Iter 1200, Minibatch Loss = 1.929803\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_5_iter-1200.cpkt\n",
      "Test loss @ iter 1200: 1.9003269672393799 \n",
      "Test Accuracy @ iter 1200: 0.1796875 \n",
      "Iter 1400, Minibatch Loss = 1.942074\n",
      "Iter 1600, Minibatch Loss = 1.909740\n",
      "Iter 1800, Minibatch Loss = 1.923509\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_5_iter-1800.cpkt\n",
      "Test loss @ iter 1800: 1.907736897468567 \n",
      "Test Accuracy @ iter 1800: 0.1953125 \n",
      "Iter 2000, Minibatch Loss = 1.913437\n",
      "Iter 2200, Minibatch Loss = 1.934298\n",
      "Iter 2400, Minibatch Loss = 1.913987\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_5_iter-2400.cpkt\n",
      "Test loss @ iter 2400: 1.9419749975204468 \n",
      "Test Accuracy @ iter 2400: 0.1875 \n",
      "Iter 2600, Minibatch Loss = 1.928038\n",
      "Iter 2800, Minibatch Loss = 1.917863\n",
      "Iter 3000, Minibatch Loss = 1.912089\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_5_iter-3000.cpkt\n",
      "Test loss @ iter 3000: 1.9379910230636597 \n",
      "Test Accuracy @ iter 3000: 0.15625 \n",
      "Iter 3200, Minibatch Loss = 1.917563\n",
      "Iter 3400, Minibatch Loss = 1.903846\n",
      "Iter 3600, Minibatch Loss = 1.926118\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_5_iter-3600.cpkt\n",
      "Test loss @ iter 3600: 1.928099274635315 \n",
      "Test Accuracy @ iter 3600: 0.1484375 \n",
      "Iter 3800, Minibatch Loss = 1.929659\n",
      "Iter 4000, Minibatch Loss = 1.919114\n",
      "Iter 4200, Minibatch Loss = 1.915242\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_5_iter-4200.cpkt\n",
      "Test loss @ iter 4200: 1.930159568786621 \n",
      "Test Accuracy @ iter 4200: 0.1953125 \n",
      "Iter 4400, Minibatch Loss = 1.932966\n",
      "Iter 4600, Minibatch Loss = 1.897269\n",
      "Iter 4800, Minibatch Loss = 1.906563\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_5_iter-4800.cpkt\n",
      "Test loss @ iter 4800: 1.8990671634674072 \n",
      "Test Accuracy @ iter 4800: 0.2109375 \n",
      "Iter 5000, Minibatch Loss = 1.931429\n",
      "Iter 5200, Minibatch Loss = 1.924710\n",
      "Iter 5400, Minibatch Loss = 1.949281\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_5_iter-5400.cpkt\n",
      "Test loss @ iter 5400: 1.9253644943237305 \n",
      "Test Accuracy @ iter 5400: 0.15625 \n",
      "Iter 5600, Minibatch Loss = 1.923844\n",
      "Iter 5800, Minibatch Loss = 1.922983\n",
      "Iter 6000, Minibatch Loss = 1.939875\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_5_iter-6000.cpkt\n",
      "Test loss @ iter 6000: 1.9163053035736084 \n",
      "Test Accuracy @ iter 6000: 0.21875 \n",
      "task  6\n",
      "Loaded 10000 data samples from qa6_yes-no-questions_train.txt\n",
      "Loaded 1000 data samples from qa6_yes-no-questions_test.txt\n",
      "Context Lengths: max = 162, min = 18, avg = 44\n",
      "Answer Lengths: max = 1, min = 1, avg = 1\n",
      "Iter 200, Minibatch Loss = 0.708248\n",
      "Iter 400, Minibatch Loss = 0.693196\n",
      "Iter 600, Minibatch Loss = 0.697494\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_6_iter-600.cpkt\n",
      "Test loss @ iter 600: 0.6889961957931519 \n",
      "Test Accuracy @ iter 600: 0.578125 \n",
      "Iter 800, Minibatch Loss = 0.696401\n",
      "Iter 1000, Minibatch Loss = 0.691690\n",
      "Iter 1200, Minibatch Loss = 0.696776\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_6_iter-1200.cpkt\n",
      "Test loss @ iter 1200: 0.693757951259613 \n",
      "Test Accuracy @ iter 1200: 0.4921875 \n",
      "Iter 1400, Minibatch Loss = 0.697737\n",
      "Iter 1600, Minibatch Loss = 0.692900\n",
      "Iter 1800, Minibatch Loss = 0.696421\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_6_iter-1800.cpkt\n",
      "Test loss @ iter 1800: 0.6926969885826111 \n",
      "Test Accuracy @ iter 1800: 0.515625 \n",
      "Iter 2000, Minibatch Loss = 0.691215\n",
      "Iter 2200, Minibatch Loss = 0.702209\n",
      "Iter 2400, Minibatch Loss = 0.699682\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_6_iter-2400.cpkt\n",
      "Test loss @ iter 2400: 0.6953223943710327 \n",
      "Test Accuracy @ iter 2400: 0.4609375 \n",
      "Iter 2600, Minibatch Loss = 0.691879\n",
      "Iter 2800, Minibatch Loss = 0.697562\n",
      "Iter 3000, Minibatch Loss = 0.691421\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_6_iter-3000.cpkt\n",
      "Test loss @ iter 3000: 0.6953727602958679 \n",
      "Test Accuracy @ iter 3000: 0.4765625 \n",
      "Iter 3200, Minibatch Loss = 0.694562\n",
      "Iter 3400, Minibatch Loss = 0.686474\n",
      "Iter 3600, Minibatch Loss = 0.695024\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_6_iter-3600.cpkt\n",
      "Test loss @ iter 3600: 0.6969813704490662 \n",
      "Test Accuracy @ iter 3600: 0.4453125 \n",
      "Iter 3800, Minibatch Loss = 0.694791\n",
      "Iter 4000, Minibatch Loss = 0.697694\n",
      "Iter 4200, Minibatch Loss = 0.694208\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_6_iter-4200.cpkt\n",
      "Test loss @ iter 4200: 0.6997061371803284 \n",
      "Test Accuracy @ iter 4200: 0.46875 \n",
      "Iter 4400, Minibatch Loss = 0.693389\n",
      "Iter 4600, Minibatch Loss = 0.695987\n",
      "Iter 4800, Minibatch Loss = 0.696097\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_6_iter-4800.cpkt\n",
      "Test loss @ iter 4800: 0.6965803503990173 \n",
      "Test Accuracy @ iter 4800: 0.4453125 \n",
      "Iter 5000, Minibatch Loss = 0.693343\n",
      "Iter 5200, Minibatch Loss = 0.691271\n",
      "Iter 5400, Minibatch Loss = 0.691370\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_6_iter-5400.cpkt\n",
      "Test loss @ iter 5400: 0.6921992301940918 \n",
      "Test Accuracy @ iter 5400: 0.53125 \n",
      "Iter 5600, Minibatch Loss = 0.692714\n",
      "Iter 5800, Minibatch Loss = 0.691018\n",
      "Iter 6000, Minibatch Loss = 0.696178\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_6_iter-6000.cpkt\n",
      "Test loss @ iter 6000: 0.6939340829849243 \n",
      "Test Accuracy @ iter 6000: 0.4921875 \n",
      "task  7\n",
      "Loaded 10000 data samples from qa7_counting_train.txt\n",
      "Loaded 1000 data samples from qa7_counting_test.txt\n",
      "Context Lengths: max = 328, min = 18, avg = 61\n",
      "Answer Lengths: max = 1, min = 1, avg = 1\n",
      "Iter 200, Minibatch Loss = 0.834678\n",
      "Iter 400, Minibatch Loss = 0.814466\n",
      "Iter 600, Minibatch Loss = 0.911929\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_7_iter-600.cpkt\n",
      "Test loss @ iter 600: 0.8514415621757507 \n",
      "Test Accuracy @ iter 600: 0.5 \n",
      "Iter 800, Minibatch Loss = 0.852216\n",
      "Iter 1000, Minibatch Loss = 0.802224\n",
      "Iter 1200, Minibatch Loss = 0.859291\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_7_iter-1200.cpkt\n",
      "Test loss @ iter 1200: 0.8706111311912537 \n",
      "Test Accuracy @ iter 1200: 0.4765625 \n",
      "Iter 1400, Minibatch Loss = 0.839446\n",
      "Iter 1600, Minibatch Loss = 0.849989\n",
      "Iter 1800, Minibatch Loss = 0.828554\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_7_iter-1800.cpkt\n",
      "Test loss @ iter 1800: 0.8594638109207153 \n",
      "Test Accuracy @ iter 1800: 0.46875 \n",
      "Iter 2000, Minibatch Loss = 0.878924\n",
      "Iter 2200, Minibatch Loss = 0.838876\n",
      "Iter 2400, Minibatch Loss = 0.911537\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_7_iter-2400.cpkt\n",
      "Test loss @ iter 2400: 0.8965288996696472 \n",
      "Test Accuracy @ iter 2400: 0.4140625 \n",
      "Iter 2600, Minibatch Loss = 0.841825\n",
      "Iter 2800, Minibatch Loss = 0.827519\n",
      "Iter 3000, Minibatch Loss = 0.803993\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_7_iter-3000.cpkt\n",
      "Test loss @ iter 3000: 0.8529782295227051 \n",
      "Test Accuracy @ iter 3000: 0.4765625 \n",
      "Iter 3200, Minibatch Loss = 0.853309\n",
      "Iter 3400, Minibatch Loss = 0.894579\n",
      "Iter 3600, Minibatch Loss = 0.827083\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_7_iter-3600.cpkt\n",
      "Test loss @ iter 3600: 0.8198105096817017 \n",
      "Test Accuracy @ iter 3600: 0.515625 \n",
      "Iter 3800, Minibatch Loss = 0.932412\n",
      "Iter 4000, Minibatch Loss = 0.868184\n",
      "Iter 4200, Minibatch Loss = 0.812881\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_7_iter-4200.cpkt\n",
      "Test loss @ iter 4200: 0.833355188369751 \n",
      "Test Accuracy @ iter 4200: 0.46875 \n",
      "Iter 4400, Minibatch Loss = 0.829827\n",
      "Iter 4600, Minibatch Loss = 0.846004\n",
      "Iter 4800, Minibatch Loss = 0.851013\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_7_iter-4800.cpkt\n",
      "Test loss @ iter 4800: 0.796917200088501 \n",
      "Test Accuracy @ iter 4800: 0.5234375 \n",
      "Iter 5000, Minibatch Loss = 0.799674\n",
      "Iter 5200, Minibatch Loss = 0.803796\n",
      "Iter 5400, Minibatch Loss = 0.850303\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_7_iter-5400.cpkt\n",
      "Test loss @ iter 5400: 0.8325510025024414 \n",
      "Test Accuracy @ iter 5400: 0.4921875 \n",
      "Iter 5600, Minibatch Loss = 0.888026\n",
      "Iter 5800, Minibatch Loss = 0.893970\n",
      "Iter 6000, Minibatch Loss = 0.898424\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_7_iter-6000.cpkt\n",
      "Test loss @ iter 6000: 0.8883222937583923 \n",
      "Test Accuracy @ iter 6000: 0.453125 \n",
      "task  8\n",
      "Loaded 10000 data samples from qa8_lists-sets_train.txt\n",
      "Loaded 1000 data samples from qa8_lists-sets_test.txt\n",
      "Context Lengths: max = 314, min = 16, avg = 58\n",
      "Answer Lengths: max = 5, min = 1, avg = 1\n",
      "Iter 200, Minibatch Loss = 0.435428\n",
      "Iter 400, Minibatch Loss = 0.354043\n",
      "Iter 600, Minibatch Loss = 0.343861\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_8_iter-600.cpkt\n",
      "Test loss @ iter 600: 0.3167082667350769 \n",
      "Test Accuracy @ iter 600: 0.375 \n",
      "Iter 800, Minibatch Loss = 0.360184\n",
      "Iter 1000, Minibatch Loss = 0.362306\n",
      "Iter 1200, Minibatch Loss = 0.346550\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_8_iter-1200.cpkt\n",
      "Test loss @ iter 1200: 0.34992140531539917 \n",
      "Test Accuracy @ iter 1200: 0.34375 \n",
      "Iter 1400, Minibatch Loss = 0.324863\n",
      "Iter 1600, Minibatch Loss = 0.327263\n",
      "Iter 1800, Minibatch Loss = 0.358436\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_8_iter-1800.cpkt\n",
      "Test loss @ iter 1800: 0.33969321846961975 \n",
      "Test Accuracy @ iter 1800: 0.3515625 \n",
      "Iter 2000, Minibatch Loss = 0.332854\n",
      "Iter 2200, Minibatch Loss = 0.356282\n",
      "Iter 2400, Minibatch Loss = 0.345584\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_8_iter-2400.cpkt\n",
      "Test loss @ iter 2400: 0.35478121042251587 \n",
      "Test Accuracy @ iter 2400: 0.265625 \n",
      "Iter 2600, Minibatch Loss = 0.333435\n",
      "Iter 2800, Minibatch Loss = 0.319060\n",
      "Iter 3000, Minibatch Loss = 0.351814\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_8_iter-3000.cpkt\n",
      "Test loss @ iter 3000: 0.31425756216049194 \n",
      "Test Accuracy @ iter 3000: 0.3359375 \n",
      "Iter 3200, Minibatch Loss = 0.335819\n",
      "Iter 3400, Minibatch Loss = 0.344821\n",
      "Iter 3600, Minibatch Loss = 0.329649\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_8_iter-3600.cpkt\n",
      "Test loss @ iter 3600: 0.3385803997516632 \n",
      "Test Accuracy @ iter 3600: 0.3125 \n",
      "Iter 3800, Minibatch Loss = 0.344389\n",
      "Iter 4000, Minibatch Loss = 0.326824\n",
      "Iter 4200, Minibatch Loss = 0.332766\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_8_iter-4200.cpkt\n",
      "Test loss @ iter 4200: 0.35897356271743774 \n",
      "Test Accuracy @ iter 4200: 0.328125 \n",
      "Iter 4400, Minibatch Loss = 0.318003\n",
      "Iter 4600, Minibatch Loss = 0.346562\n",
      "Iter 4800, Minibatch Loss = 0.339503\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_8_iter-4800.cpkt\n",
      "Test loss @ iter 4800: 0.3069867193698883 \n",
      "Test Accuracy @ iter 4800: 0.3359375 \n",
      "Iter 5000, Minibatch Loss = 0.336619\n",
      "Iter 5200, Minibatch Loss = 0.319366\n",
      "Iter 5400, Minibatch Loss = 0.325998\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_8_iter-5400.cpkt\n",
      "Test loss @ iter 5400: 0.3437000513076782 \n",
      "Test Accuracy @ iter 5400: 0.2890625 \n",
      "Iter 5600, Minibatch Loss = 0.331525\n",
      "Iter 5800, Minibatch Loss = 0.309907\n",
      "Iter 6000, Minibatch Loss = 0.348043\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_8_iter-6000.cpkt\n",
      "Test loss @ iter 6000: 0.34114307165145874 \n",
      "Test Accuracy @ iter 6000: 0.3125 \n",
      "task  9\n",
      "Loaded 10000 data samples from qa9_simple-negation_train.txt\n",
      "Loaded 1000 data samples from qa9_simple-negation_test.txt\n",
      "Context Lengths: max = 80, min = 18, avg = 44\n",
      "Answer Lengths: max = 1, min = 1, avg = 1\n",
      "Iter 200, Minibatch Loss = 0.682290\n",
      "Iter 400, Minibatch Loss = 0.671111\n",
      "Iter 600, Minibatch Loss = 0.653008\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_9_iter-600.cpkt\n",
      "Test loss @ iter 600: 0.6984601020812988 \n",
      "Test Accuracy @ iter 600: 0.5625 \n",
      "Iter 800, Minibatch Loss = 0.660729\n",
      "Iter 1000, Minibatch Loss = 0.659779\n",
      "Iter 1200, Minibatch Loss = 0.688361\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_9_iter-1200.cpkt\n",
      "Test loss @ iter 1200: 0.6320145130157471 \n",
      "Test Accuracy @ iter 1200: 0.6875 \n",
      "Iter 1400, Minibatch Loss = 0.663043\n",
      "Iter 1600, Minibatch Loss = 0.674075\n",
      "Iter 1800, Minibatch Loss = 0.665107\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_9_iter-1800.cpkt\n",
      "Test loss @ iter 1800: 0.6471673250198364 \n",
      "Test Accuracy @ iter 1800: 0.671875 \n",
      "Iter 2000, Minibatch Loss = 0.676924\n",
      "Iter 2200, Minibatch Loss = 0.654113\n",
      "Iter 2400, Minibatch Loss = 0.603977\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_9_iter-2400.cpkt\n",
      "Test loss @ iter 2400: 0.613401472568512 \n",
      "Test Accuracy @ iter 2400: 0.640625 \n",
      "Iter 2600, Minibatch Loss = 0.610042\n",
      "Iter 2800, Minibatch Loss = 0.632439\n",
      "Iter 3000, Minibatch Loss = 0.609074\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_9_iter-3000.cpkt\n",
      "Test loss @ iter 3000: 0.6316579580307007 \n",
      "Test Accuracy @ iter 3000: 0.6328125 \n",
      "Iter 3200, Minibatch Loss = 0.564291\n",
      "Iter 3400, Minibatch Loss = 0.513065\n",
      "Iter 3600, Minibatch Loss = 0.471233\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_9_iter-3600.cpkt\n",
      "Test loss @ iter 3600: 0.7196683883666992 \n",
      "Test Accuracy @ iter 3600: 0.6171875 \n",
      "Iter 3800, Minibatch Loss = 0.419125\n",
      "Iter 4000, Minibatch Loss = 0.294287\n",
      "Iter 4200, Minibatch Loss = 0.239875\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_9_iter-4200.cpkt\n",
      "Test loss @ iter 4200: 0.7754013538360596 \n",
      "Test Accuracy @ iter 4200: 0.7265625 \n",
      "Iter 4400, Minibatch Loss = 0.253855\n",
      "Iter 4600, Minibatch Loss = 0.199313\n",
      "Iter 4800, Minibatch Loss = 0.141636\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_9_iter-4800.cpkt\n",
      "Test loss @ iter 4800: 0.8582665920257568 \n",
      "Test Accuracy @ iter 4800: 0.765625 \n",
      "Iter 5000, Minibatch Loss = 0.123229\n",
      "Iter 5200, Minibatch Loss = 0.096979\n",
      "Iter 5400, Minibatch Loss = 0.117733\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_9_iter-5400.cpkt\n",
      "Test loss @ iter 5400: 1.176020622253418 \n",
      "Test Accuracy @ iter 5400: 0.7578125 \n",
      "Iter 5600, Minibatch Loss = 0.079598\n",
      "Iter 5800, Minibatch Loss = 0.020628\n",
      "Iter 6000, Minibatch Loss = 0.078297\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_9_iter-6000.cpkt\n",
      "Test loss @ iter 6000: 1.6445739269256592 \n",
      "Test Accuracy @ iter 6000: 0.7109375 \n",
      "task  10\n",
      "Loaded 10000 data samples from qa10_indefinite-knowledge_train.txt\n",
      "Loaded 1000 data samples from qa10_indefinite-knowledge_test.txt\n",
      "Context Lengths: max = 95, min = 18, avg = 48\n",
      "Answer Lengths: max = 1, min = 1, avg = 1\n",
      "Iter 200, Minibatch Loss = 1.004164\n",
      "Iter 400, Minibatch Loss = 1.019549\n",
      "Iter 600, Minibatch Loss = 1.016678\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_10_iter-600.cpkt\n",
      "Test loss @ iter 600: 0.9909443259239197 \n",
      "Test Accuracy @ iter 600: 0.5 \n",
      "Iter 800, Minibatch Loss = 0.990674\n",
      "Iter 1000, Minibatch Loss = 0.920833\n",
      "Iter 1200, Minibatch Loss = 0.933652\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_10_iter-1200.cpkt\n",
      "Test loss @ iter 1200: 0.8837990760803223 \n",
      "Test Accuracy @ iter 1200: 0.4921875 \n",
      "Iter 1400, Minibatch Loss = 0.877504\n",
      "Iter 1600, Minibatch Loss = 0.873813\n",
      "Iter 1800, Minibatch Loss = 0.871925\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_10_iter-1800.cpkt\n",
      "Test loss @ iter 1800: 0.9669474363327026 \n",
      "Test Accuracy @ iter 1800: 0.375 \n",
      "Iter 2000, Minibatch Loss = 0.877478\n",
      "Iter 2200, Minibatch Loss = 0.842886\n",
      "Iter 2400, Minibatch Loss = 0.760986\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_10_iter-2400.cpkt\n",
      "Test loss @ iter 2400: 0.8170117139816284 \n",
      "Test Accuracy @ iter 2400: 0.4921875 \n",
      "Iter 2600, Minibatch Loss = 0.703761\n",
      "Iter 2800, Minibatch Loss = 0.649674\n",
      "Iter 3000, Minibatch Loss = 0.627055\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_10_iter-3000.cpkt\n",
      "Test loss @ iter 3000: 0.7686606645584106 \n",
      "Test Accuracy @ iter 3000: 0.6015625 \n",
      "Iter 3200, Minibatch Loss = 0.478339\n",
      "Iter 3400, Minibatch Loss = 0.520211\n",
      "Iter 3600, Minibatch Loss = 0.360099\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_10_iter-3600.cpkt\n",
      "Test loss @ iter 3600: 1.0691583156585693 \n",
      "Test Accuracy @ iter 3600: 0.625 \n",
      "Iter 3800, Minibatch Loss = 0.395021\n",
      "Iter 4000, Minibatch Loss = 0.270894\n",
      "Iter 4200, Minibatch Loss = 0.265465\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_10_iter-4200.cpkt\n",
      "Test loss @ iter 4200: 1.474840521812439 \n",
      "Test Accuracy @ iter 4200: 0.6015625 \n",
      "Iter 4400, Minibatch Loss = 0.235938\n",
      "Iter 4600, Minibatch Loss = 0.195378\n",
      "Iter 4800, Minibatch Loss = 0.173277\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_10_iter-4800.cpkt\n",
      "Test loss @ iter 4800: 1.3074333667755127 \n",
      "Test Accuracy @ iter 4800: 0.6875 \n",
      "Iter 5000, Minibatch Loss = 0.193863\n",
      "Iter 5200, Minibatch Loss = 0.170215\n",
      "Iter 5400, Minibatch Loss = 0.131434\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_10_iter-5400.cpkt\n",
      "Test loss @ iter 5400: 1.36332368850708 \n",
      "Test Accuracy @ iter 5400: 0.6875 \n",
      "Iter 5600, Minibatch Loss = 0.119281\n",
      "Iter 5800, Minibatch Loss = 0.115091\n",
      "Iter 6000, Minibatch Loss = 0.112757\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_10_iter-6000.cpkt\n",
      "Test loss @ iter 6000: 2.0510199069976807 \n",
      "Test Accuracy @ iter 6000: 0.5859375 \n",
      "task  11\n",
      "Loaded 10000 data samples from qa11_basic-coreference_train.txt\n",
      "Loaded 1000 data samples from qa11_basic-coreference_test.txt\n",
      "Context Lengths: max = 79, min = 17, avg = 45\n",
      "Answer Lengths: max = 1, min = 1, avg = 1\n",
      "Iter 200, Minibatch Loss = 1.828443\n",
      "Iter 400, Minibatch Loss = 1.472610\n",
      "Iter 600, Minibatch Loss = 1.266138\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_11_iter-600.cpkt\n",
      "Test loss @ iter 600: 1.0045182704925537 \n",
      "Test Accuracy @ iter 600: 0.5625 \n",
      "Iter 800, Minibatch Loss = 0.917533\n",
      "Iter 1000, Minibatch Loss = 0.726623\n",
      "Iter 1200, Minibatch Loss = 0.615802\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_11_iter-1200.cpkt\n",
      "Test loss @ iter 1200: 0.7059433460235596 \n",
      "Test Accuracy @ iter 1200: 0.7109375 \n",
      "Iter 1400, Minibatch Loss = 0.527431\n",
      "Iter 1600, Minibatch Loss = 0.314503\n",
      "Iter 1800, Minibatch Loss = 0.364189\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_11_iter-1800.cpkt\n",
      "Test loss @ iter 1800: 0.34298500418663025 \n",
      "Test Accuracy @ iter 1800: 0.90625 \n",
      "Iter 2000, Minibatch Loss = 0.326590\n",
      "Iter 2200, Minibatch Loss = 0.291786\n",
      "Iter 2400, Minibatch Loss = 0.244998\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_11_iter-2400.cpkt\n",
      "Test loss @ iter 2400: 0.39452242851257324 \n",
      "Test Accuracy @ iter 2400: 0.8671875 \n",
      "Iter 2600, Minibatch Loss = 0.204608\n",
      "Iter 2800, Minibatch Loss = 0.140361\n",
      "Iter 3000, Minibatch Loss = 0.087032\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_11_iter-3000.cpkt\n",
      "Test loss @ iter 3000: 0.32976338267326355 \n",
      "Test Accuracy @ iter 3000: 0.9140625 \n",
      "Iter 3200, Minibatch Loss = 0.025479\n",
      "Iter 3400, Minibatch Loss = 0.019945\n",
      "Iter 3600, Minibatch Loss = 0.030631\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_11_iter-3600.cpkt\n",
      "Test loss @ iter 3600: 0.18567056953907013 \n",
      "Test Accuracy @ iter 3600: 0.9453125 \n",
      "Iter 3800, Minibatch Loss = 0.031585\n",
      "Iter 4000, Minibatch Loss = 0.056381\n",
      "Iter 4200, Minibatch Loss = 0.061689\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_11_iter-4200.cpkt\n",
      "Test loss @ iter 4200: 0.3692723512649536 \n",
      "Test Accuracy @ iter 4200: 0.921875 \n",
      "Iter 4400, Minibatch Loss = 0.007396\n",
      "Iter 4600, Minibatch Loss = 0.025356\n",
      "Iter 4800, Minibatch Loss = 0.015828\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_11_iter-4800.cpkt\n",
      "Test loss @ iter 4800: 0.2019764482975006 \n",
      "Test Accuracy @ iter 4800: 0.953125 \n",
      "Iter 5000, Minibatch Loss = 0.003939\n",
      "Iter 5200, Minibatch Loss = 0.023454\n",
      "Iter 5400, Minibatch Loss = 0.037946\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_11_iter-5400.cpkt\n",
      "Test loss @ iter 5400: 0.3689900040626526 \n",
      "Test Accuracy @ iter 5400: 0.9375 \n",
      "Iter 5600, Minibatch Loss = 0.006007\n",
      "Iter 5800, Minibatch Loss = 0.015143\n",
      "Iter 6000, Minibatch Loss = 0.012000\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_11_iter-6000.cpkt\n",
      "Test loss @ iter 6000: 0.44524896144866943 \n",
      "Test Accuracy @ iter 6000: 0.953125 \n",
      "task  12\n",
      "Loaded 10000 data samples from qa12_conjunction_train.txt\n",
      "Loaded 1000 data samples from qa12_conjunction_test.txt\n",
      "Context Lengths: max = 91, min = 20, avg = 53\n",
      "Answer Lengths: max = 1, min = 1, avg = 1\n",
      "Iter 200, Minibatch Loss = 1.815821\n",
      "Iter 400, Minibatch Loss = 1.542603\n",
      "Iter 600, Minibatch Loss = 1.064131\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_12_iter-600.cpkt\n",
      "Test loss @ iter 600: 1.0039794445037842 \n",
      "Test Accuracy @ iter 600: 0.7734375 \n",
      "Iter 800, Minibatch Loss = 0.899144\n",
      "Iter 1000, Minibatch Loss = 0.756939\n",
      "Iter 1200, Minibatch Loss = 0.775647\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_12_iter-1200.cpkt\n",
      "Test loss @ iter 1200: 0.7407019138336182 \n",
      "Test Accuracy @ iter 1200: 0.765625 \n",
      "Iter 1400, Minibatch Loss = 0.721066\n",
      "Iter 1600, Minibatch Loss = 0.785968\n",
      "Iter 1800, Minibatch Loss = 0.685509\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_12_iter-1800.cpkt\n",
      "Test loss @ iter 1800: 0.7007886171340942 \n",
      "Test Accuracy @ iter 1800: 0.78125 \n",
      "Iter 2000, Minibatch Loss = 0.560658\n",
      "Iter 2200, Minibatch Loss = 0.100435\n",
      "Iter 2400, Minibatch Loss = 0.091572\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_12_iter-2400.cpkt\n",
      "Test loss @ iter 2400: 0.13205590844154358 \n",
      "Test Accuracy @ iter 2400: 0.96875 \n",
      "Iter 2600, Minibatch Loss = 0.032755\n",
      "Iter 2800, Minibatch Loss = 0.020510\n",
      "Iter 3000, Minibatch Loss = 0.037496\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_12_iter-3000.cpkt\n",
      "Test loss @ iter 3000: 0.2514617145061493 \n",
      "Test Accuracy @ iter 3000: 0.953125 \n",
      "Iter 3200, Minibatch Loss = 0.016159\n",
      "Iter 3400, Minibatch Loss = 0.026764\n",
      "Iter 3600, Minibatch Loss = 0.066222\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_12_iter-3600.cpkt\n",
      "Test loss @ iter 3600: 0.1509799361228943 \n",
      "Test Accuracy @ iter 3600: 0.96875 \n",
      "Iter 3800, Minibatch Loss = 0.014930\n",
      "Iter 4000, Minibatch Loss = 0.002378\n",
      "Iter 4200, Minibatch Loss = 0.007134\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_12_iter-4200.cpkt\n",
      "Test loss @ iter 4200: 0.03013617917895317 \n",
      "Test Accuracy @ iter 4200: 0.9921875 \n",
      "Iter 4400, Minibatch Loss = 0.004495\n",
      "Iter 4600, Minibatch Loss = 0.044094\n",
      "Iter 4800, Minibatch Loss = 0.000953\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_12_iter-4800.cpkt\n",
      "Test loss @ iter 4800: 0.01882392168045044 \n",
      "Test Accuracy @ iter 4800: 0.9921875 \n",
      "Iter 5000, Minibatch Loss = 0.002584\n",
      "Iter 5200, Minibatch Loss = 0.026306\n",
      "Iter 5400, Minibatch Loss = 0.001430\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_12_iter-5400.cpkt\n",
      "Test loss @ iter 5400: 0.1820138692855835 \n",
      "Test Accuracy @ iter 5400: 0.9765625 \n",
      "Iter 5600, Minibatch Loss = 0.001291\n",
      "Iter 5800, Minibatch Loss = 0.003457\n",
      "Iter 6000, Minibatch Loss = 0.001984\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_12_iter-6000.cpkt\n",
      "Test loss @ iter 6000: 0.32858580350875854 \n",
      "Test Accuracy @ iter 6000: 0.9609375 \n",
      "task  13\n",
      "Loaded 10000 data samples from qa13_compound-coreference_train.txt\n",
      "Loaded 1000 data samples from qa13_compound-coreference_test.txt\n",
      "Context Lengths: max = 90, min = 19, avg = 51\n",
      "Answer Lengths: max = 1, min = 1, avg = 1\n",
      "Iter 200, Minibatch Loss = 1.852089\n",
      "Iter 400, Minibatch Loss = 1.798489\n",
      "Iter 600, Minibatch Loss = 1.008492\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_13_iter-600.cpkt\n",
      "Test loss @ iter 600: 0.9689513444900513 \n",
      "Test Accuracy @ iter 600: 0.4375 \n",
      "Iter 800, Minibatch Loss = 0.402033\n",
      "Iter 1000, Minibatch Loss = 0.348307\n",
      "Iter 1200, Minibatch Loss = 0.270826\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_13_iter-1200.cpkt\n",
      "Test loss @ iter 1200: 0.20012947916984558 \n",
      "Test Accuracy @ iter 1200: 0.96875 \n",
      "Iter 1400, Minibatch Loss = 0.289919\n",
      "Iter 1600, Minibatch Loss = 0.345038\n",
      "Iter 1800, Minibatch Loss = 0.229075\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_13_iter-1800.cpkt\n",
      "Test loss @ iter 1800: 0.3140677809715271 \n",
      "Test Accuracy @ iter 1800: 0.9140625 \n",
      "Iter 2000, Minibatch Loss = 0.177977\n",
      "Iter 2200, Minibatch Loss = 0.152632\n",
      "Iter 2400, Minibatch Loss = 0.256561\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_13_iter-2400.cpkt\n",
      "Test loss @ iter 2400: 0.1737389713525772 \n",
      "Test Accuracy @ iter 2400: 0.953125 \n",
      "Iter 2600, Minibatch Loss = 0.219749\n",
      "Iter 2800, Minibatch Loss = 0.199450\n",
      "Iter 3000, Minibatch Loss = 0.157501\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_13_iter-3000.cpkt\n",
      "Test loss @ iter 3000: 0.10018035024404526 \n",
      "Test Accuracy @ iter 3000: 0.984375 \n",
      "Iter 3200, Minibatch Loss = 0.208915\n",
      "Iter 3400, Minibatch Loss = 0.119711\n",
      "Iter 3600, Minibatch Loss = 0.119049\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_13_iter-3600.cpkt\n",
      "Test loss @ iter 3600: 0.4128069579601288 \n",
      "Test Accuracy @ iter 3600: 0.921875 \n",
      "Iter 3800, Minibatch Loss = 0.132528\n",
      "Iter 4000, Minibatch Loss = 0.051716\n",
      "Iter 4200, Minibatch Loss = 0.056138\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_13_iter-4200.cpkt\n",
      "Test loss @ iter 4200: 0.4953193664550781 \n",
      "Test Accuracy @ iter 4200: 0.875 \n",
      "Iter 4400, Minibatch Loss = 0.050546\n",
      "Iter 4600, Minibatch Loss = 0.071503\n",
      "Iter 4800, Minibatch Loss = 0.036525\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_13_iter-4800.cpkt\n",
      "Test loss @ iter 4800: 0.4122154116630554 \n",
      "Test Accuracy @ iter 4800: 0.890625 \n",
      "Iter 5000, Minibatch Loss = 0.034358\n",
      "Iter 5200, Minibatch Loss = 0.038396\n",
      "Iter 5400, Minibatch Loss = 0.045643\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_13_iter-5400.cpkt\n",
      "Test loss @ iter 5400: 0.4602311849594116 \n",
      "Test Accuracy @ iter 5400: 0.90625 \n",
      "Iter 5600, Minibatch Loss = 0.019777\n",
      "Iter 5800, Minibatch Loss = 0.004266\n",
      "Iter 6000, Minibatch Loss = 0.024205\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_13_iter-6000.cpkt\n",
      "Test loss @ iter 6000: 1.0526645183563232 \n",
      "Test Accuracy @ iter 6000: 0.859375 \n",
      "task  14\n",
      "Loaded 10000 data samples from qa14_time-reasoning_train.txt\n",
      "Loaded 1000 data samples from qa14_time-reasoning_test.txt\n",
      "Context Lengths: max = 123, min = 22, avg = 66\n",
      "Answer Lengths: max = 1, min = 1, avg = 1\n",
      "Iter 200, Minibatch Loss = 1.817410\n",
      "Iter 400, Minibatch Loss = 1.826009\n",
      "Iter 600, Minibatch Loss = 1.786680\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_14_iter-600.cpkt\n",
      "Test loss @ iter 600: 1.7969744205474854 \n",
      "Test Accuracy @ iter 600: 0.1953125 \n",
      "Iter 800, Minibatch Loss = 1.790737\n",
      "Iter 1000, Minibatch Loss = 1.796812\n",
      "Iter 1200, Minibatch Loss = 1.789456\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_14_iter-1200.cpkt\n",
      "Test loss @ iter 1200: 1.7902846336364746 \n",
      "Test Accuracy @ iter 1200: 0.1796875 \n",
      "Iter 1400, Minibatch Loss = 1.795403\n",
      "Iter 1600, Minibatch Loss = 1.798406\n",
      "Iter 1800, Minibatch Loss = 1.789915\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_14_iter-1800.cpkt\n",
      "Test loss @ iter 1800: 1.7906821966171265 \n",
      "Test Accuracy @ iter 1800: 0.1171875 \n",
      "Iter 2000, Minibatch Loss = 1.793603\n",
      "Iter 2200, Minibatch Loss = 1.795265\n",
      "Iter 2400, Minibatch Loss = 1.793514\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_14_iter-2400.cpkt\n",
      "Test loss @ iter 2400: 1.7946453094482422 \n",
      "Test Accuracy @ iter 2400: 0.1953125 \n",
      "Iter 2600, Minibatch Loss = 1.795767\n",
      "Iter 2800, Minibatch Loss = 1.796715\n",
      "Iter 3000, Minibatch Loss = 1.794373\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_14_iter-3000.cpkt\n",
      "Test loss @ iter 3000: 1.792243480682373 \n",
      "Test Accuracy @ iter 3000: 0.140625 \n",
      "Iter 3200, Minibatch Loss = 1.794707\n",
      "Iter 3400, Minibatch Loss = 1.797645\n",
      "Iter 3600, Minibatch Loss = 1.781906\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_14_iter-3600.cpkt\n",
      "Test loss @ iter 3600: 1.8029370307922363 \n",
      "Test Accuracy @ iter 3600: 0.1484375 \n",
      "Iter 3800, Minibatch Loss = 1.756038\n",
      "Iter 4000, Minibatch Loss = 1.799633\n",
      "Iter 4200, Minibatch Loss = 1.774321\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_14_iter-4200.cpkt\n",
      "Test loss @ iter 4200: 1.7425888776779175 \n",
      "Test Accuracy @ iter 4200: 0.1953125 \n",
      "Iter 4400, Minibatch Loss = 1.777235\n",
      "Iter 4600, Minibatch Loss = 1.765956\n",
      "Iter 4800, Minibatch Loss = 1.707558\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_14_iter-4800.cpkt\n",
      "Test loss @ iter 4800: 1.670337200164795 \n",
      "Test Accuracy @ iter 4800: 0.2421875 \n",
      "Iter 5000, Minibatch Loss = 1.600425\n",
      "Iter 5200, Minibatch Loss = 1.557435\n",
      "Iter 5400, Minibatch Loss = 1.507851\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_14_iter-5400.cpkt\n",
      "Test loss @ iter 5400: 1.6365137100219727 \n",
      "Test Accuracy @ iter 5400: 0.3125 \n",
      "Iter 5600, Minibatch Loss = 1.471519\n",
      "Iter 5800, Minibatch Loss = 1.395660\n",
      "Iter 6000, Minibatch Loss = 1.314746\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_14_iter-6000.cpkt\n",
      "Test loss @ iter 6000: 1.4940123558044434 \n",
      "Test Accuracy @ iter 6000: 0.3671875 \n",
      "task  15\n",
      "Loaded 10000 data samples from qa15_basic-deduction_train.txt\n",
      "Loaded 1000 data samples from qa15_basic-deduction_test.txt\n",
      "Context Lengths: max = 50, min = 50, avg = 50\n",
      "Answer Lengths: max = 1, min = 1, avg = 1\n",
      "Iter 200, Minibatch Loss = 1.406798\n",
      "Iter 400, Minibatch Loss = 1.394088\n",
      "Iter 600, Minibatch Loss = 1.298928\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_15_iter-600.cpkt\n",
      "Test loss @ iter 600: 1.2944152355194092 \n",
      "Test Accuracy @ iter 600: 0.3671875 \n",
      "Iter 800, Minibatch Loss = 1.176760\n",
      "Iter 1000, Minibatch Loss = 1.156853\n",
      "Iter 1200, Minibatch Loss = 0.908074\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_15_iter-1200.cpkt\n",
      "Test loss @ iter 1200: 0.9421149492263794 \n",
      "Test Accuracy @ iter 1200: 0.5859375 \n",
      "Iter 1400, Minibatch Loss = 0.852648\n",
      "Iter 1600, Minibatch Loss = 0.816938\n",
      "Iter 1800, Minibatch Loss = 0.774294\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_15_iter-1800.cpkt\n",
      "Test loss @ iter 1800: 0.9372639656066895 \n",
      "Test Accuracy @ iter 1800: 0.5859375 \n",
      "Iter 2000, Minibatch Loss = 0.738020\n",
      "Iter 2200, Minibatch Loss = 0.627183\n",
      "Iter 2400, Minibatch Loss = 0.655169\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_15_iter-2400.cpkt\n",
      "Test loss @ iter 2400: 0.7417141795158386 \n",
      "Test Accuracy @ iter 2400: 0.671875 \n",
      "Iter 2600, Minibatch Loss = 0.584926\n",
      "Iter 2800, Minibatch Loss = 0.526410\n",
      "Iter 3000, Minibatch Loss = 0.481552\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_15_iter-3000.cpkt\n",
      "Test loss @ iter 3000: 1.3701097965240479 \n",
      "Test Accuracy @ iter 3000: 0.5234375 \n",
      "Iter 3200, Minibatch Loss = 0.437695\n",
      "Iter 3400, Minibatch Loss = 0.331379\n",
      "Iter 3600, Minibatch Loss = 0.288794\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_15_iter-3600.cpkt\n",
      "Test loss @ iter 3600: 1.7881505489349365 \n",
      "Test Accuracy @ iter 3600: 0.515625 \n",
      "Iter 3800, Minibatch Loss = 0.240452\n",
      "Iter 4000, Minibatch Loss = 0.251194\n",
      "Iter 4200, Minibatch Loss = 0.211389\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_15_iter-4200.cpkt\n",
      "Test loss @ iter 4200: 2.2193703651428223 \n",
      "Test Accuracy @ iter 4200: 0.5546875 \n",
      "Iter 4400, Minibatch Loss = 0.124070\n",
      "Iter 4600, Minibatch Loss = 0.149792\n",
      "Iter 4800, Minibatch Loss = 0.101928\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_15_iter-4800.cpkt\n",
      "Test loss @ iter 4800: 2.9700992107391357 \n",
      "Test Accuracy @ iter 4800: 0.453125 \n",
      "Iter 5000, Minibatch Loss = 0.153269\n",
      "Iter 5200, Minibatch Loss = 0.135493\n",
      "Iter 5400, Minibatch Loss = 0.075136\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_15_iter-5400.cpkt\n",
      "Test loss @ iter 5400: 2.8403966426849365 \n",
      "Test Accuracy @ iter 5400: 0.515625 \n",
      "Iter 5600, Minibatch Loss = 0.074906\n",
      "Iter 5800, Minibatch Loss = 0.110375\n",
      "Iter 6000, Minibatch Loss = 0.063068\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_15_iter-6000.cpkt\n",
      "Test loss @ iter 6000: 2.945369243621826 \n",
      "Test Accuracy @ iter 6000: 0.53125 \n",
      "task  16\n",
      "Loaded 10000 data samples from qa16_basic-induction_train.txt\n",
      "Loaded 1000 data samples from qa16_basic-induction_test.txt\n",
      "Context Lengths: max = 46, min = 46, avg = 46\n",
      "Answer Lengths: max = 1, min = 1, avg = 1\n",
      "Iter 200, Minibatch Loss = 0.951491\n",
      "Iter 400, Minibatch Loss = 0.896523\n",
      "Iter 600, Minibatch Loss = 0.907218\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_16_iter-600.cpkt\n",
      "Test loss @ iter 600: 0.8859198689460754 \n",
      "Test Accuracy @ iter 600: 0.5078125 \n",
      "Iter 800, Minibatch Loss = 0.896979\n",
      "Iter 1000, Minibatch Loss = 0.858894\n",
      "Iter 1200, Minibatch Loss = 0.842605\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_16_iter-1200.cpkt\n",
      "Test loss @ iter 1200: 0.9298388957977295 \n",
      "Test Accuracy @ iter 1200: 0.453125 \n",
      "Iter 1400, Minibatch Loss = 0.835239\n",
      "Iter 1600, Minibatch Loss = 0.794324\n",
      "Iter 1800, Minibatch Loss = 0.754702\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_16_iter-1800.cpkt\n",
      "Test loss @ iter 1800: 1.0591068267822266 \n",
      "Test Accuracy @ iter 1800: 0.4296875 \n",
      "Iter 2000, Minibatch Loss = 0.750641\n",
      "Iter 2200, Minibatch Loss = 0.693555\n",
      "Iter 2400, Minibatch Loss = 0.714215\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_16_iter-2400.cpkt\n",
      "Test loss @ iter 2400: 1.0652878284454346 \n",
      "Test Accuracy @ iter 2400: 0.46875 \n",
      "Iter 2600, Minibatch Loss = 0.627659\n",
      "Iter 2800, Minibatch Loss = 0.505958\n",
      "Iter 3000, Minibatch Loss = 0.560692\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_16_iter-3000.cpkt\n",
      "Test loss @ iter 3000: 1.9103686809539795 \n",
      "Test Accuracy @ iter 3000: 0.4140625 \n",
      "Iter 3200, Minibatch Loss = 0.377969\n",
      "Iter 3400, Minibatch Loss = 0.280279\n",
      "Iter 3600, Minibatch Loss = 0.232662\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_16_iter-3600.cpkt\n",
      "Test loss @ iter 3600: 2.072786808013916 \n",
      "Test Accuracy @ iter 3600: 0.4453125 \n",
      "Iter 3800, Minibatch Loss = 0.195299\n",
      "Iter 4000, Minibatch Loss = 0.191559\n",
      "Iter 4200, Minibatch Loss = 0.188550\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_16_iter-4200.cpkt\n",
      "Test loss @ iter 4200: 2.4246749877929688 \n",
      "Test Accuracy @ iter 4200: 0.484375 \n",
      "Iter 4400, Minibatch Loss = 0.176059\n",
      "Iter 4600, Minibatch Loss = 0.154970\n",
      "Iter 4800, Minibatch Loss = 0.081469\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_16_iter-4800.cpkt\n",
      "Test loss @ iter 4800: 3.2055087089538574 \n",
      "Test Accuracy @ iter 4800: 0.4375 \n",
      "Iter 5000, Minibatch Loss = 0.097829\n",
      "Iter 5200, Minibatch Loss = 0.050293\n",
      "Iter 5400, Minibatch Loss = 0.091714\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_16_iter-5400.cpkt\n",
      "Test loss @ iter 5400: 3.057097911834717 \n",
      "Test Accuracy @ iter 5400: 0.5234375 \n",
      "Iter 5600, Minibatch Loss = 0.119030\n",
      "Iter 5800, Minibatch Loss = 0.068571\n",
      "Iter 6000, Minibatch Loss = 0.087112\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_16_iter-6000.cpkt\n",
      "Test loss @ iter 6000: 3.1664140224456787 \n",
      "Test Accuracy @ iter 6000: 0.453125 \n",
      "task  17\n",
      "Loaded 10000 data samples from qa17_positional-reasoning_train.txt\n",
      "Loaded 1000 data samples from qa17_positional-reasoning_test.txt\n",
      "Context Lengths: max = 36, min = 25, avg = 30\n",
      "Answer Lengths: max = 1, min = 1, avg = 1\n",
      "Iter 200, Minibatch Loss = 0.653640\n",
      "Iter 400, Minibatch Loss = 0.589411\n",
      "Iter 600, Minibatch Loss = 0.600126\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_17_iter-600.cpkt\n",
      "Test loss @ iter 600: 0.5840158462524414 \n",
      "Test Accuracy @ iter 600: 0.5859375 \n",
      "Iter 800, Minibatch Loss = 0.583657\n",
      "Iter 1000, Minibatch Loss = 0.583915\n",
      "Iter 1200, Minibatch Loss = 0.605312\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_17_iter-1200.cpkt\n",
      "Test loss @ iter 1200: 0.6550869941711426 \n",
      "Test Accuracy @ iter 1200: 0.546875 \n",
      "Iter 1400, Minibatch Loss = 0.575781\n",
      "Iter 1600, Minibatch Loss = 0.559724\n",
      "Iter 1800, Minibatch Loss = 0.436202\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_17_iter-1800.cpkt\n",
      "Test loss @ iter 1800: 0.5123147964477539 \n",
      "Test Accuracy @ iter 1800: 0.7109375 \n",
      "Iter 2000, Minibatch Loss = 0.367588\n",
      "Iter 2200, Minibatch Loss = 0.351723\n",
      "Iter 2400, Minibatch Loss = 0.316032\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_17_iter-2400.cpkt\n",
      "Test loss @ iter 2400: 0.443181574344635 \n",
      "Test Accuracy @ iter 2400: 0.7734375 \n",
      "Iter 2600, Minibatch Loss = 0.292896\n",
      "Iter 2800, Minibatch Loss = 0.274112\n",
      "Iter 3000, Minibatch Loss = 0.290976\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_17_iter-3000.cpkt\n",
      "Test loss @ iter 3000: 0.6587775945663452 \n",
      "Test Accuracy @ iter 3000: 0.7578125 \n",
      "Iter 3200, Minibatch Loss = 0.219065\n",
      "Iter 3400, Minibatch Loss = 0.195075\n",
      "Iter 3600, Minibatch Loss = 0.191150\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_17_iter-3600.cpkt\n",
      "Test loss @ iter 3600: 0.42406603693962097 \n",
      "Test Accuracy @ iter 3600: 0.8515625 \n",
      "Iter 3800, Minibatch Loss = 0.067941\n",
      "Iter 4000, Minibatch Loss = 0.107898\n",
      "Iter 4200, Minibatch Loss = 0.045672\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_17_iter-4200.cpkt\n",
      "Test loss @ iter 4200: 1.0486271381378174 \n",
      "Test Accuracy @ iter 4200: 0.765625 \n",
      "Iter 4400, Minibatch Loss = 0.023744\n",
      "Iter 4600, Minibatch Loss = 0.012100\n",
      "Iter 4800, Minibatch Loss = 0.036108\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_17_iter-4800.cpkt\n",
      "Test loss @ iter 4800: 1.4426779747009277 \n",
      "Test Accuracy @ iter 4800: 0.7578125 \n",
      "Iter 5000, Minibatch Loss = 0.017245\n",
      "Iter 5200, Minibatch Loss = 0.022887\n",
      "Iter 5400, Minibatch Loss = 0.015741\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_17_iter-5400.cpkt\n",
      "Test loss @ iter 5400: 1.1101834774017334 \n",
      "Test Accuracy @ iter 5400: 0.828125 \n",
      "Iter 5600, Minibatch Loss = 0.032584\n",
      "Iter 5800, Minibatch Loss = 0.002458\n",
      "Iter 6000, Minibatch Loss = 0.013060\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_17_iter-6000.cpkt\n",
      "Test loss @ iter 6000: 0.8678090572357178 \n",
      "Test Accuracy @ iter 6000: 0.8515625 \n",
      "task  18\n",
      "Loaded 10000 data samples from qa18_size-reasoning_train.txt\n",
      "Loaded 1000 data samples from qa18_size-reasoning_test.txt\n",
      "Context Lengths: max = 179, min = 43, avg = 60\n",
      "Answer Lengths: max = 1, min = 1, avg = 1\n",
      "Iter 200, Minibatch Loss = 0.698266\n",
      "Iter 400, Minibatch Loss = 0.698212\n",
      "Iter 600, Minibatch Loss = 0.701105\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_18_iter-600.cpkt\n",
      "Test loss @ iter 600: 0.6958128213882446 \n",
      "Test Accuracy @ iter 600: 0.484375 \n",
      "Iter 800, Minibatch Loss = 0.689155\n",
      "Iter 1000, Minibatch Loss = 0.694405\n",
      "Iter 1200, Minibatch Loss = 0.691785\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_18_iter-1200.cpkt\n",
      "Test loss @ iter 1200: 0.6924024224281311 \n",
      "Test Accuracy @ iter 1200: 0.546875 \n",
      "Iter 1400, Minibatch Loss = 0.695198\n",
      "Iter 1600, Minibatch Loss = 0.695112\n",
      "Iter 1800, Minibatch Loss = 0.691460\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_18_iter-1800.cpkt\n",
      "Test loss @ iter 1800: 0.6949506998062134 \n",
      "Test Accuracy @ iter 1800: 0.4453125 \n",
      "Iter 2000, Minibatch Loss = 0.692175\n",
      "Iter 2200, Minibatch Loss = 0.691131\n",
      "Iter 2400, Minibatch Loss = 0.695055\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_18_iter-2400.cpkt\n",
      "Test loss @ iter 2400: 0.6918975710868835 \n",
      "Test Accuracy @ iter 2400: 0.53125 \n",
      "Iter 2600, Minibatch Loss = 0.693490\n",
      "Iter 2800, Minibatch Loss = 0.695656\n",
      "Iter 3000, Minibatch Loss = 0.696723\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_18_iter-3000.cpkt\n",
      "Test loss @ iter 3000: 0.6930358409881592 \n",
      "Test Accuracy @ iter 3000: 0.5234375 \n",
      "Iter 3200, Minibatch Loss = 0.694825\n",
      "Iter 3400, Minibatch Loss = 0.692475\n",
      "Iter 3600, Minibatch Loss = 0.690711\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_18_iter-3600.cpkt\n",
      "Test loss @ iter 3600: 0.6922625303268433 \n",
      "Test Accuracy @ iter 3600: 0.5234375 \n",
      "Iter 3800, Minibatch Loss = 0.694365\n",
      "Iter 4000, Minibatch Loss = 0.691346\n",
      "Iter 4200, Minibatch Loss = 0.696636\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_18_iter-4200.cpkt\n",
      "Test loss @ iter 4200: 0.6887831687927246 \n",
      "Test Accuracy @ iter 4200: 0.5546875 \n",
      "Iter 4400, Minibatch Loss = 0.698532\n",
      "Iter 4600, Minibatch Loss = 0.693164\n",
      "Iter 4800, Minibatch Loss = 0.693105\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_18_iter-4800.cpkt\n",
      "Test loss @ iter 4800: 0.6927745342254639 \n",
      "Test Accuracy @ iter 4800: 0.546875 \n",
      "Iter 5000, Minibatch Loss = 0.693241\n",
      "Iter 5200, Minibatch Loss = 0.694638\n",
      "Iter 5400, Minibatch Loss = 0.694725\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_18_iter-5400.cpkt\n",
      "Test loss @ iter 5400: 0.695412278175354 \n",
      "Test Accuracy @ iter 5400: 0.46875 \n",
      "Iter 5600, Minibatch Loss = 0.693770\n",
      "Iter 5800, Minibatch Loss = 0.171473\n",
      "Iter 6000, Minibatch Loss = 0.116717\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_18_iter-6000.cpkt\n",
      "Test loss @ iter 6000: 0.14374299347400665 \n",
      "Test Accuracy @ iter 6000: 0.890625 \n",
      "task  19\n",
      "Loaded 10000 data samples from qa19_path-finding_train.txt\n",
      "Loaded 1000 data samples from qa19_path-finding_test.txt\n",
      "Context Lengths: max = 51, min = 51, avg = 51\n",
      "Answer Lengths: max = 2, min = 2, avg = 2\n",
      "Iter 200, Minibatch Loss = 1.351637\n",
      "Iter 400, Minibatch Loss = 1.281042\n",
      "Iter 600, Minibatch Loss = 1.272308\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_19_iter-600.cpkt\n",
      "Test loss @ iter 600: 1.2390530109405518 \n",
      "Test Accuracy @ iter 600: 0.09375 \n",
      "Iter 800, Minibatch Loss = 1.248101\n",
      "Iter 1000, Minibatch Loss = 1.228895\n",
      "Iter 1200, Minibatch Loss = 1.238498\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_19_iter-1200.cpkt\n",
      "Test loss @ iter 1200: 1.2454291582107544 \n",
      "Test Accuracy @ iter 1200: 0.078125 \n",
      "Iter 1400, Minibatch Loss = 1.237920\n",
      "Iter 1600, Minibatch Loss = 1.215943\n",
      "Iter 1800, Minibatch Loss = 1.157711\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_19_iter-1800.cpkt\n",
      "Test loss @ iter 1800: 1.1940268278121948 \n",
      "Test Accuracy @ iter 1800: 0.109375 \n",
      "Iter 2000, Minibatch Loss = 1.125033\n",
      "Iter 2200, Minibatch Loss = 1.042491\n",
      "Iter 2400, Minibatch Loss = 1.004063\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_19_iter-2400.cpkt\n",
      "Test loss @ iter 2400: 1.07444429397583 \n",
      "Test Accuracy @ iter 2400: 0.1796875 \n",
      "Iter 2600, Minibatch Loss = 0.914578\n",
      "Iter 2800, Minibatch Loss = 0.831626\n",
      "Iter 3000, Minibatch Loss = 0.820832\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_19_iter-3000.cpkt\n",
      "Test loss @ iter 3000: 1.1374335289001465 \n",
      "Test Accuracy @ iter 3000: 0.2265625 \n",
      "Iter 3200, Minibatch Loss = 0.767048\n",
      "Iter 3400, Minibatch Loss = 0.705389\n",
      "Iter 3600, Minibatch Loss = 0.654924\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_19_iter-3600.cpkt\n",
      "Test loss @ iter 3600: 1.3445799350738525 \n",
      "Test Accuracy @ iter 3600: 0.2109375 \n",
      "Iter 3800, Minibatch Loss = 0.644022\n",
      "Iter 4000, Minibatch Loss = 0.612141\n",
      "Iter 4200, Minibatch Loss = 0.653316\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_19_iter-4200.cpkt\n",
      "Test loss @ iter 4200: 1.3520734310150146 \n",
      "Test Accuracy @ iter 4200: 0.2109375 \n",
      "Iter 4400, Minibatch Loss = 0.628526\n",
      "Iter 4600, Minibatch Loss = 0.601842\n",
      "Iter 4800, Minibatch Loss = 0.600554\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_19_iter-4800.cpkt\n",
      "Test loss @ iter 4800: 1.3100262880325317 \n",
      "Test Accuracy @ iter 4800: 0.25 \n",
      "Iter 5000, Minibatch Loss = 0.600929\n",
      "Iter 5200, Minibatch Loss = 0.600336\n",
      "Iter 5400, Minibatch Loss = 0.601445\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_19_iter-5400.cpkt\n",
      "Test loss @ iter 5400: 1.1864579916000366 \n",
      "Test Accuracy @ iter 5400: 0.2578125 \n",
      "Iter 5600, Minibatch Loss = 0.586380\n",
      "Iter 5800, Minibatch Loss = 0.575006\n",
      "Iter 6000, Minibatch Loss = 0.533518\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_19_iter-6000.cpkt\n",
      "Test loss @ iter 6000: 1.5438233613967896 \n",
      "Test Accuracy @ iter 6000: 0.2265625 \n",
      "task  20\n",
      "Loaded 10000 data samples from qa20_agents-motivations_train.txt\n",
      "Loaded 1000 data samples from qa20_agents-motivations_test.txt\n",
      "Context Lengths: max = 76, min = 9, avg = 36\n",
      "Answer Lengths: max = 1, min = 1, avg = 1\n",
      "Iter 200, Minibatch Loss = 0.570970\n",
      "Iter 400, Minibatch Loss = 0.170684\n",
      "Iter 600, Minibatch Loss = 0.115555\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_20_iter-600.cpkt\n",
      "Test loss @ iter 600: 0.04205469787120819 \n",
      "Test Accuracy @ iter 600: 0.9921875 \n",
      "Iter 800, Minibatch Loss = 0.086656\n",
      "Iter 1000, Minibatch Loss = 0.035087\n",
      "Iter 1200, Minibatch Loss = 0.041743\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_20_iter-1200.cpkt\n",
      "Test loss @ iter 1200: 0.014528105966746807 \n",
      "Test Accuracy @ iter 1200: 1.0 \n",
      "Iter 1400, Minibatch Loss = 0.047419\n",
      "Iter 1600, Minibatch Loss = 0.050403\n",
      "Iter 1800, Minibatch Loss = 0.052671\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_20_iter-1800.cpkt\n",
      "Test loss @ iter 1800: 0.02535632997751236 \n",
      "Test Accuracy @ iter 1800: 0.9921875 \n",
      "Iter 2000, Minibatch Loss = 0.026022\n",
      "Iter 2200, Minibatch Loss = 0.028505\n",
      "Iter 2400, Minibatch Loss = 0.040088\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_20_iter-2400.cpkt\n",
      "Test loss @ iter 2400: 0.014236729592084885 \n",
      "Test Accuracy @ iter 2400: 1.0 \n",
      "Iter 2600, Minibatch Loss = 0.024177\n",
      "Iter 2800, Minibatch Loss = 0.011967\n",
      "Iter 3000, Minibatch Loss = 0.023847\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_20_iter-3000.cpkt\n",
      "Test loss @ iter 3000: 0.042694926261901855 \n",
      "Test Accuracy @ iter 3000: 0.9765625 \n",
      "Iter 3200, Minibatch Loss = 0.038390\n",
      "Iter 3400, Minibatch Loss = 0.019151\n",
      "Iter 3600, Minibatch Loss = 0.026950\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_20_iter-3600.cpkt\n",
      "Test loss @ iter 3600: 0.02876984141767025 \n",
      "Test Accuracy @ iter 3600: 0.984375 \n",
      "Iter 3800, Minibatch Loss = 0.038153\n",
      "Iter 4000, Minibatch Loss = 0.017206\n",
      "Iter 4200, Minibatch Loss = 0.024376\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_20_iter-4200.cpkt\n",
      "Test loss @ iter 4200: 0.09100347757339478 \n",
      "Test Accuracy @ iter 4200: 0.9609375 \n",
      "Iter 4400, Minibatch Loss = 0.006540\n",
      "Iter 4600, Minibatch Loss = 0.005751\n",
      "Iter 4800, Minibatch Loss = 0.033255\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_20_iter-4800.cpkt\n",
      "Test loss @ iter 4800: 0.03836551308631897 \n",
      "Test Accuracy @ iter 4800: 0.9765625 \n",
      "Iter 5000, Minibatch Loss = 0.000763\n",
      "Iter 5200, Minibatch Loss = 0.019601\n",
      "Iter 5400, Minibatch Loss = 0.009021\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_20_iter-5400.cpkt\n",
      "Test loss @ iter 5400: 0.0113596823066473 \n",
      "Test Accuracy @ iter 5400: 1.0 \n",
      "Iter 5600, Minibatch Loss = 0.001037\n",
      "Iter 5800, Minibatch Loss = 0.002095\n",
      "Iter 6000, Minibatch Loss = 0.000700\n",
      "Model saved in file: /home/ai2-rey/Documents/code/Nikhita/deep_QA/weights/bAbI_QA_20_iter-6000.cpkt\n",
      "Test loss @ iter 6000: 0.09346146881580353 \n",
      "Test Accuracy @ iter 6000: 0.984375 \n"
     ]
    }
   ],
   "source": [
    "for task in range(1,21):\n",
    "    print(\"task \", task)\n",
    "    data_dir = '/media/ai2-rey/data_disk/data_sets/bAbI/tasks_1-20_v1-2/en-10k/'\n",
    "    LSTM_data = BabiDataset(data_dir, task, 'seq2seq')\n",
    "    \n",
    "    train_data = list(zip(LSTM_data.train_input_tokens, LSTM_data.train_labels_tokens))\n",
    "    test_data = list(zip(LSTM_data.test_input_tokens, LSTM_data.test_labels_tokens))\n",
    "    \n",
    "    train_data_iter = DataIterator(train_data, 256)\n",
    "    test_data_iter = DataIterator(test_data, 128)\n",
    "    deploy_data_iter = DataIterator(test_data, 1)\n",
    "    \n",
    "    vocab_size = len(LSTM_data.vocab) \n",
    "\n",
    "    xseq_len = LSTM_data.max_context_len\n",
    "    yseq_len = LSTM_data.max_answer_len\n",
    "\n",
    "    num_layers = 3\n",
    "    lr_rate = 0.001\n",
    "    momentum = 0.9\n",
    "    n_hidden = 256 \n",
    "    word_dim = 100\n",
    "    dropout_rate = 0.5\n",
    "    gpu_device = 0\n",
    "    model_dir = None\n",
    "    \n",
    "    model = Seq2Seq(xseq_len=xseq_len,\n",
    "               yseq_len=yseq_len, \n",
    "               vocab_size=vocab_size,\n",
    "               word_dim = word_dim, \n",
    "               num_layers = num_layers,\n",
    "               dropout_rate=0.5,\n",
    "               gpu_device = gpu_device)\n",
    "    \n",
    "    sess = model.train(train_data_iter, test_data_iter, deploy_data_iter,\n",
    "                      train_iters=6000, display_step=200,\n",
    "                      save_weights_interval = 600, id2word_dict=LSTM_data.id2word_dict, \n",
    "                      weights_prefix='bAbI_QA_{}'.format(LSTM_data.task_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 data samples from qa20_agents-motivations_train.txt\n",
      "Loaded 1000 data samples from qa20_agents-motivations_test.txt\n",
      "Context Lengths: max = 76, min = 9, avg = 36\n",
      "Answer Lengths: max = 1, min = 1, avg = 1\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/media/ai2-rey/data_disk/data_sets/bAbI/tasks_1-20_v1-2/en-10k/'\n",
    "LSTM_data = BabiDataset(data_dir, 20, 'seq2seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = list(zip(LSTM_data.train_input_tokens, LSTM_data.train_labels_tokens))\n",
    "test_data = list(zip(LSTM_data.test_input_tokens, LSTM_data.test_labels_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_iter = DataIterator(train_data, 256)\n",
    "test_data_iter = DataIterator(test_data, 999)\n",
    "deploy_data_iter = DataIterator(test_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(LSTM_data.vocab) \n",
    "\n",
    "xseq_len = LSTM_data.max_context_len\n",
    "yseq_len = LSTM_data.max_answer_len\n",
    " \n",
    "num_layers = 3\n",
    "lr_rate = 0.001\n",
    "momentum = 0.9\n",
    "n_hidden = 256 \n",
    "word_dim = 100\n",
    "dropout_rate = 0.5\n",
    "gpu_device = 1\n",
    "model_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Seq2Seq(xseq_len=xseq_len,\n",
    "               yseq_len=yseq_len, \n",
    "               vocab_size=vocab_size,\n",
    "               word_dim = word_dim, \n",
    "               num_layers = num_layers,\n",
    "               dropout_rate=0.5,\n",
    "               gpu_device = gpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy = model.test_try('bAbI_QA_8_iter-200.cpkt', test_data_iter)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = model.train(train_data_iter, test_data_iter, deploy_data_iter,\n",
    "                  train_iters=1000, display_step=200,\n",
    "                  save_weights_interval = 200, id2word_dict=LSTM_data.id2word_dict, \n",
    "                  weights_prefix='bAbI_QA_{}'.format(LSTM_data.task_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i, l = test_data_iter.next_batch()\n",
    "o = model.predict('bAbI_QA_20_iter-6000.cpkt', i)\n",
    "print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = l.T[0]\n",
    "\" \".join([LSTM_data.id2word_dict[q] for q in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ii, tl, oi in list(zip(i.T, l.T, o)):\n",
    "    inp_string = \" \".join([LSTM_data.id2word_dict[q] for q in ii])\n",
    "    true_label = \" \".join([LSTM_data.id2word_dict[t] for t in tl])\n",
    "    predict_string = \" \".join([LSTM_data.id2word_dict[o] for o in oi])\n",
    "    print('q:{} \\n prediction:{} \\n true label:{}'.format(inp_string,predict_string, true_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LSTM_data.test_labels_tokens[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LSTM_data.id2word_dict[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = test_data_iter.next_batch()\n",
    "o = model.predict(sess, i)\n",
    "print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LSTM_data.train_labels_raw[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LSTM_data.train_labels_tokens[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for a in LSTM_data.test_input_tokens[11]:\n",
    "    print(LSTM_data.id2word_dict[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(LSTM_data.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LSTM_data.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xseq_length = LSTM_data.max_context_len\n",
    "yseq_length = LSTM_data.max_answer_len\n",
    "batch_size = 64\n",
    "vocab_size = len(LSTM_data.vocab)\n",
    "embedding_dim = 50\n",
    "learning_rate = 0.0001\n",
    "dropout_rate = 0.5\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encode_input = [tf.placeholder(tf.int32, \n",
    "                                shape=(None,),\n",
    "                                name = \"ei_%i\" %i)\n",
    "                                for i in range(xseq_length)]\n",
    "\n",
    "labels = [tf.placeholder(tf.int32,\n",
    "                         shape=(None,),\n",
    "                         name = \"l_%i\" %i)\n",
    "          for i in range(yseq_length)]\n",
    "\n",
    "decode_input = [tf.zeros_like(encode_input[0], dtype=np.int32, name=\"GO\")] + labels[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "cells = [rnn_cell.DropoutWrapper(\n",
    "        rnn_cell.BasicLSTMCell(embedding_dim), output_keep_prob=keep_prob\n",
    "    ) for i in range(3)]\n",
    "\n",
    "stacked_lstm = rnn_cell.MultiRNNCell(cells)\n",
    "\n",
    "with tf.variable_scope(\"decoders\") as scope:\n",
    "    decode_outputs, decode_state = tf.nn.seq2seq.embedding_rnn_seq2seq(\n",
    "        encode_input, decode_input, stacked_lstm, vocab_size, vocab_size, embedding_dim)\n",
    "    \n",
    "    scope.reuse_variables()\n",
    "    \n",
    "    decode_outputs_test, decode_state_test = tf.nn.seq2seq.embedding_rnn_seq2seq(\n",
    "        encode_input, decode_input, stacked_lstm, vocab_size, vocab_size, embedding_dim,\n",
    "    feed_previous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_weights = [tf.ones_like(l, dtype=tf.float32) for l in labels]\n",
    "loss = tf.nn.seq2seq.sequence_loss(decode_outputs, labels, loss_weights, vocab_size)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feed(X, Y):\n",
    "    feed_dict = {encode_input[t]: X[t] for t in range(xseq_length)}\n",
    "    feed_dict.update({labels[t]: Y[t] for t in range(yseq_length)})\n",
    "    return feed_dict\n",
    "\n",
    "def train_batch(data_iter):\n",
    "    X, Y = data_iter.next_batch()\n",
    "    feed_dict = get_feed(X, Y)\n",
    "    feed_dict[keep_prob] = dropout_rate\n",
    "    _, out = sess.run([train_op, loss], feed_dict)\n",
    "    return out\n",
    "\n",
    "def get_eval_batch_data(data_iter):\n",
    "    X, Y = data_iter.next_batch()\n",
    "    feed_dict = get_feed(X, Y)\n",
    "    feed_dict[keep_prob] = 1.\n",
    "    all_output = sess.run([loss] + decode_outputs_test, feed_dict)\n",
    "    eval_loss = all_output[0]\n",
    "    decode_output = np.array(all_output[1:]).transpose([1,0,2])\n",
    "    return eval_loss, decode_output, X, Y\n",
    "\n",
    "def eval_batch(data_iter, num_batches):\n",
    "    losses = []\n",
    "    predict_loss = []\n",
    "    for i in range(num_batches):\n",
    "        eval_loss, output, X, Y = get_eval_batch_data(data_iter)\n",
    "        losses.append(eval_loss)\n",
    "        \n",
    "        for index in range(len(output)):\n",
    "            real = Y.T[index]\n",
    "            predict = np.argmax(output, axis = 2)[index]\n",
    "            predict_loss.append(all(real==predict))\n",
    "    return np.mean(losses), np.mean(predict_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    try:\n",
    "        train_batch(train_data_iter)\n",
    "        if i % 1000 == 0:\n",
    "            val_loss, val_predict = eval_batch(test_data_iter, 16)\n",
    "            train_loss, train_predict = eval_batch(train_data_iter, 16)\n",
    "            print(\"val loss   : %f, val predict   = %.1f%%\" %(val_loss, val_predict * 100))\n",
    "            print(\"train loss : %f, train predict = %.1f%%\" %(train_loss, train_predict * 100))\n",
    "            print\n",
    "            sys.stdout.flush()\n",
    "    except KeyboardInterrupt:\n",
    "        print( \"interrupted by user\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x= np.array([[1,2,3],[4,5,6]])\n",
    "y=np.array([[1],[2]])\n",
    "batch_size = 1\n",
    "# for i in range(0, len(x), batch_size):\n",
    "#     if (i+1)*batch_size < len(x):\n",
    "#         x = x[i: (i+1)*batch_size].T\n",
    "#         y = y[i:(i+1)*batch_size].T\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_env]",
   "language": "python",
   "name": "conda-env-tensorflow_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
